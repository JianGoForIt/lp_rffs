\documentclass[12pt]{article}
\usepackage{setspace}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage[pdftex]{graphics}
\usepackage{graphicx}
\usepackage{amsthm}


\topmargin = -0.5in \textwidth=6.5in \textheight=9in

\oddsidemargin = 0in \evensidemargin = 0in

\DeclareMathSizes{12}{10}{7}{5}
\newcommand{\bphi}{\bar{\phi}}
\newcommand{\bw}{\bar{w}}
\newcommand{\bv}{\bar{v}}
\newcommand{\bF}{\bar{F}}
\newcommand{\bg}{\bar{g}}
\newcommand{\be}{\bar{\eta}}
\newcommand{\br}{\bar{\rho}}
\newcommand{\bmu}{\bar{\mu}}
\newcommand{\bx}{\bar{x}}
\newcommand{\bS}{\bar{S}}
\newcommand{\bT}{\bar{T}}
\newcommand{\sq}{\sqrt{2}}
\newcommand{\hk}{\hat{k}}
\newcommand{\hr}{\hat{r}}
\newcommand{\hy}{\hat{y}}
\newcommand{\hY}{\hat{Y}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\tW}{\tilde{W}}
\newcommand{\tb}{\tilde{b}}
\newcommand{\tg}{\tilde{g}}
\newcommand{\tz}{\tilde{z}}
\newcommand{\eps}{\epsilon}
\newcommand{\teps}{\tilde{\epsilon}}
\newcommand{\tS}{\tilde{S}}
\newcommand{\tA}{\tilde{A}}
\newcommand{\tsigma}{\tilde{\sigma}}
\newcommand{\tlambda}{\tilde{\lambda}}
\newcommand{\ulq}{\underline{q}}
\newcommand{\olq}{\overline{q}}
\newcommand{\ulr}{\underline{r}}
\newcommand{\olr}{\overline{r}}
\newcommand{\Exp}{\ProbOpr{E}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\input{__macros}
\begin{document}


\title{Low-Precision Random Fourier Features (LP-RFF)}
\author{Avner May, Jian Zhang}
\onehalfspacing
\maketitle

\section{Introduction}
\label{sec:intro}
The goal of this work is to understand whether we can train better kernel approximation models, at lower cost,
by quantizing the random Fourier features [1].  We will analyze, empirically and theoretically, the performance of low-precision random Fourier features (LP-RFF), relative to the full-precision features.  Specifically, we will consider quantizing the features, which take the form $z_i(x) = \sqrt{\frac{2}{m}} \cos(w_i^Tx+b_i)$ into $b$-bits, through some form of quantization scheme (simplest: random rounding). 
When using $b$-bits to represent values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$, we will simply divide this interval evenly into $2^b-1$ sub-intervals.  Letting $\delta = \frac{2\sqrt{2/m}}{2^b-1}$, this allows us to represent the values $\{-\sqrt{2/m},-\sqrt{2/m} + \delta,\ldots,\sqrt{2/m}-\delta, \sqrt{2/m}\}$. \\

\noindent \textbf{Outline}:
\begin{itemize}
	\item Section \ref{sec:kernel_approx}: Analysis of kernel approximation error.
	\item Section \ref{sec:gen}: Analysis of generalization performance.
	\item Section \ref{sec:train}: Training in low-precision.
	\item Section \ref{sec:experiments}: Experiments
\end{itemize}

\noindent \textbf{Main questions}:
\begin{itemize}
	\item What can we say about the variance of these low-precision features, in their approximation of the kernel?  Concentration inequalities?
	\item Can we analyze the generalization performance of these features?
	\item Is there an optimal level of precision, with regard to achieving the best kernel approximation performance, or generalization performance, under a fixed memory budget?
	\item How should model training be performed on top of these low-precision features? Can we train in low-precision, and if so, what would the effects of such a training scheme be on the performance of the model?
\end{itemize}

%I will now present some initial theoretical results.  In the section below, assume that $Q_b:\RR\rightarrow\RR$ is a random quantization function which quantizes the interval $[-\sq,\sq]$ into $b$ bits, with the property that $\expect{}{Q_b(z)} = z \; \forall b,z$.  Given this quantization, we will define a ``quantization interval'' to be the set of points between two consecutive quantized values.
%%For $z\in [-\sq,\sq]$, let $\ulq(z)$ be the ``bottom'' of the quantization interval containing $z$, let $\olq(z)$ denote the ``top'' of this interval.
%
%\subsection{Initial Theoretical Results}
%\begin{itemize}
%%	\item \noindent\textbf{Definitions}: For $z \in [a,c]$, let $X_z^{a,c}$ be the random variable which with probability $\frac{z-a}{c-a}$ equals $c-z$, and with probability $\frac{c-z}{c-a}$ equals $a-z$. Furthermore, let $Q^{a,c}(z) = z + X_z^{a,c} \in \{a,c\}$ be the ``quantized'' version of $z$, corresponding to randomized rounding to $a$ or $c$.
%	\item \noindent\textbf{Proposition 1}: Let $Q$ be any unbiased quantization function with bounded variance ($\expect{}{Q(z)} = z$, $\var{}{Q(z)} \leq \tsigma^2$ for any $z$).  Let $S=Z_x Z_y$, $T = Q(Z_x)Q(Z_y)$, and $(S_1,\ldots,S_n)$, $(T_1,\ldots,T_n)$ be a random sequence of i.i.d. draws from $S$ and $T$ respectively.  Define $\bar{S}_n = \frac{1}{n}\sum_{i=1}^n S_i$, and $\bar{T}_n =  \frac{1}{n}\sum_{i=1}^n T_i$, to be the empirical mean of these draws.  It follows that $\expect{}{\bS_n} = \expect{}{\bT_n} = k(x,y)$, and that
%	$\var{}{\bS_n} = \frac{\sigma^2}{n}$, and $\var{}{\bT_n} \leq \frac{2\tsigma^2 + \tsigma^4 +  \sigma^2}{n}$.  Furthermore, in the context of using random rounding for random Fourier features, $\tsigma^2 \leq \frac{2}{(2^b-1)^2}$. 
%	\item \textbf{Proposition 2} (Concentration bounds for LP-RFF): For a fixed $x,y\in\cX$, let $S$ be any random variable with the property that $\expect{}{S} = k(x,y)$, and $S \in [-2,2]$.\footnote{$S=Q_b(\sq\cos(w^Tx+b))Q_b(\sq\cos(w^Ty+b))$ satisfies these conditions.}  Let $(S_1,\ldots,S_n)$, be a random sequence of i.i.d. draws from $S$, and let $\bS_n = \frac{1}{n}\sum_{i=1}^n S_i$ be the empirical mean of this sequence.  Then, it follows directly from Hoeffding's inequality that $\Prob{\big[|\bS_n - k(x,y)| \geq \epsilon\big]} \leq 2\exp(-n\eps^2/8)$.
%\end{itemize}
%\subsubsection{Remarks}
%\begin{enumerate}
%\item To better understand Proposition 1, we now plot upper bounds for $\frac{\sigma^2}{n}$ and $\frac{\sigma^2 + \tsigma^2}{n}$, using $\sigma^2\leq 1$, and $\tsigma^2 \leq  \frac{2}{(2^b-1)^2}$; we use a log-log plot, and plot these upper bounds for various values of $b$ and $n$.\\
%**OUTDATED**
%\begin{center}
%	\includegraphics[width=0.6\textwidth]{lprff_variance_figure_numbits.jpg}
%\end{center}
%As you can see, lowering the precision helps reduce the variance by approximately a full order of magnitude ($10\times$ smaller variance) for a fixed number of bits, with $b=2$ giving the lowest variance, and $b=32$ variance matching the full-precision line (they are overlapping in the figure).  In the plot below, we show the kernel approximation variance as a function of the number of features.  As you can see, using $1$ bit per feature gives a noticeable jump in variance, but all other higher precision features perform comparably to the full-precision random features.
%**OUTDATED**
%\begin{center}
%	\includegraphics[width=0.6\textwidth]{lprff_variance_figure_numfeat.jpg}
%\end{center}
% FIGURES
%figure; hold on;
%n = 1:100000;
%for b = [1,2,3,4,8,16,32]
%plot(n,(  (1/2) + 2/(2^(2*b)-2^(b+1)+1)  ) ./ n,'LineWidth',2)
%end
%plot(n,0.5./n,'LineWidth',2)
%lgd = legend('b=1','b=2','b=3','b=4','b=8','b=16','b=32','full-prec');
%lgd.FontSize = 14;
%xlabel('Total # features','FontSize',14)
%ylabel('variance','FontSize',14)
%title('Kernel approximation variance vs. number of features','FontSize',14)
%set(gca,'yscale','log')
%set(gca,'xscale','log')
%
%figure; hold on;
%n = 1:100000;
%for b = [1,2,3,4,8,16,32]
%plot(n,(  (1/2) + 2/(2^(2*b)-2^(b+1)+1)  ) ./ ((32/b) * n),'LineWidth',2)
%end
%plot(n,0.5./n,'LineWidth',2)
%lgd = legend('b=1','b=2','b=3','b=4','b=8','b=16','b=32','full-prec');
%lgd.FontSize = 14;
%xlabel('Total # bits / 32','FontSize',14)
%ylabel('variance','FontSize',14)
%title('Kernel approximation variance vs. memory usage','FontSize',14)
%set(gca,'yscale','log')
%set(gca,'xscale','log')
%



%\item It is important to note that Propositions 2 and 3 hold as a function of the number of low-precision features, and \textit{match} the bounds for the \textit{same number} of high-precision features.\footnote{Some care is needed for Proposition 3, because the set of functions $\cF_p$ we are considering is different than the one in the original.  I believe I can show that for a properly defined set of quantization functions, our $\cF_p$ is a superset of the original, though I may need to update the constant $C$.}  This is pretty amazing!
%\end{enumerate}

%\subsection{Open Questions}
%\begin{enumerate}
%	%\item Can we extend Claim 1 (Uniform convergence of Fourier features) from $[5]$ (Rahimi and Recht, 2007) to low-precision features $z(x)$, to prove that the probability that there exists $x,y\in\cX$ such that $|z(x)^Tz(y) -k(x,y)| \geq \eps$ is small?
%	%\item Can we perform training in such a way that the learned weights are also low-precision?  What guarantees can we get with a training algorithm of this form?
%	%\item How does the family of functions $\cF_p$ defined in Proposition 3 compare to the one in $[4]$ (Rahimi and Recht, 2008)?	
%	%\item What can we say about the spectrum of the low-precision features relative to the full-precision features, and the exact kernel?  Results here would allow us to apply the ``fixed design'' linear regression analysis (from my other notes) here.
%	%\item Can we use low-precision for $x$,$w$, and $b$ in the features $z(x) = \sq\cos(w^Tx+b)$?  What can we prove about this?
%\end{enumerate}

%\section{Motivation}
%Random Fourier features are an effective way of scaling kernel methods to large datasets.  Unfortunately, this method often requires a very large number of features in order to attain strong performance.  For example, in my work in speech recognition, increasing the number of features from 100k to 400k continued to give meaningful improvements.  At this scale, the amount of memory and computation time required to train models becomes a big bottleneck. For example, the size of a GPU's global memory can limit the number of random features which can be used, if one would like to train the model fully on a single GPU.  
%
%Furthermore, in my work comparing random Fourier features and the \Nystrom method, I observed that using \textit{many} random Fourier features allows for approximating a larger portion of the kernel's spectrum, which appears to be important for attaining strong performance. This is in contrast to the \Nystrom method, which under a similar computational budget, computes fewer more expensive features; although these features approximate the kernel matrix very well, they are inherently limited in how many of the kernel's eigenvalues they can approximate.  The take-away here appears to be: many cheap features is better than fewer expensive features, even if those expensive features approximate the kernel matrix very well.
%
%In light of the computational bottleneck which arises when training models with very many random features, as well as the observation that using a large number of features is important for downstream performance (even if they have higher variance when approximating the kernel), I propose using \textit{low-precision} as a way to further scale these random feature methods.
%Some challenges involved in implementing and analyzing this idea are as follows:
%
%\begin{enumerate}
%	\item What are the trade-offs in deciding the number of bits of precision which should be used for the data, the random projection matrix, the random features, and the model parameters?  Can we effectively learn models on single-bit random features?
%	\item If the model is stored in low-precision, what optimization algorithm should be used during training in order to make this possible?
%	\item Will the final model be low-precision or high-precision?  Another way of asking this is: Is low-precision a trick to speed up the training of a full-precision model (as in HALP), or will the entire system be low-precision at both train and test time?
%	\item How can we formalize the way the additional noise caused by quantization will affect the training of the model?  Can we use variance reduction techniques like SVRG to deal with this, like in HALP?
%	\item Can we somehow use this additional randomness in the feature generation process in order to generate ``error bars'' in the estimates of the trained model (\eg, by evaluating the model on several random draws of the randomly quantized features for the same data point, in order to produce a distribution of predictions)?
%\end{enumerate}
%
%I will now discuss a few ideas related to two aspects of this project: (1) computing the random features, and (2) training a model with these features.


\section{Kernel Approximation Error for LP-RFF}
\label{sec:kernel_approx}
%\noindent\textbf{Definitions}: For $z \in [a,c]$, let $X_z^{a,c}$ be the random variable which with probability $\frac{z-a}{c-a}$ equals $c-z$, and with probability $\frac{c-z}{c-a}$ equals $a-z$. 
%Furthermore, let $Q^{a,c}(z) = z + X_z^{a,c} \in \{a,c\}$ be the ``quantized'' version of $z$, corresponding to randomized rounding to $a$ or $c$.

\subsection{Kernel Approximation Variance Analysis}
\noindent\textbf{Definition}:  For $z \in [a,c]$, let $X_z^{a,c}$ be the random variable which with probability $\frac{z-a}{c-a}$ equals $c-z$, and with probability $\frac{c-z}{c-a}$ equals $a-z$.

\begin{lemma}
\label{lemma:qvar}
$\expect{}{X_z^{a,c}} = 0$, and $\var{}{X_z^{a,c}} = (z-a)(c-z) \leq \frac{(c-a)^2}{4}$.
\end{lemma}

\begin{proof}
\begin{eqnarray*}
	\expect{}{X_z^{a,c}} &=&  (c-z)\cdot \frac{z-a}{c-a} + (a-z)\cdot \frac{c-z}{c-a}\\
	&=& 0 .\\
	\var{}{X_z^{a,c}} &=& (c-z)^2\cdot \frac{z-a}{c-a} + (a-z)^2\cdot \frac{c-z}{c-a}\\
	&=& \frac{(c-z)(z-a)((c-z + z-a))}{c-a} \\
	&=& (c-z)(z-a)\\
	\frac{d}{dz}[\var{}{X_z^{a,c}}] &=& \frac{d}{dz}[-z^2 + (c+a)z -ac] \\
	&=& -2z + c+a.
\end{eqnarray*}
Now, setting the derivative to 0 gives $z^* = \frac{c+a}{2}$.  Thus, $\argmax_{z\in[a,c]} (c-z)(z-a) = \frac{c+a}{2}$, and $\max_{z\in[a,c]} (c-z)(z-a) = (c-\frac{c+a}{2})(\frac{c+a}{2}-a) = \frac{(c-a)^2}{4}$.
\end{proof}

% This theorem analyzes the variance of a single quantized random Fourier feature.
\begin{lemma}
\label{lemma:var1}
For $x,y\in\cX$, assume we have random variables $Z_x,Z_y$ satisfying $\expect{}{Z_xZ_y} = k(x,y)$, and $\var{}{Z_x} = \var{}{Z_y} = \sigma^2$, and that $k(x,x) = k(y,y) = 1$.\footnote{For example, one specific instance of the random variables $Z_x,Z_y$ is given by random Fourier features, where $z_x = \sq\cos(w^Tx+b),z_y = \sq\cos(w^Ty+b)$, $z_x,z_y\in[-\sq,\sq]$, for random $w,b$.}  For any unbiased random quantization function $Q$ with bounded variance $\var{}{Q(z)} \leq \tsigma^2$ for any $z$, it follows that $\expect{}{Q(Z_x)Q(Z_y)} = k(x,y)$, and that $\var{}{Q(Z_x)Q(Z_y)} \leq 2\tsigma^2 + \tsigma^4 + \sigma^2$.
\end{lemma}
\begin{proof}
Let $Q(Z_x) = Z_x + \eps_x$, and $Q(Z_y) = Z_y + \eps_y$, where $\expect{}{\eps_x} =\expect{}{\eps_y} = 0$ and $\expect{}{\eps_x^2} \leq \tsigma^2$, $\expect{}{\eps_y^2} \leq \tsigma^2$.

\begin{eqnarray*}
\expect{}{Q(Z_x)Q(Z_y)} &=& \expect{}{(Z_x + \eps_x)(Z_y + \eps_y)} \\
&=& \expect{}{Z_xZ_y + Z_y \eps_x + Z_x \eps_y + \eps_x \eps_y} \\
&=& \expect{}{Z_xZ_y} \\
&=& k(x,y).
\end{eqnarray*}
\begin{eqnarray*}
\var{}{Q(Z_x)Q(Z_y)} &=&  \expect{}{\Big(Q(Z_x)Q(Z_y) - k(x,y)\Big)^2} \\
&=& \expect{}{\Big((Z_x + \eps_x)(Z_y + \eps_y) - k(x,y)\Big)^2} \\
&=& \expect{}{\Big(Z_y \eps_x + Z_x\eps_y + \eps_x \eps_y +  Z_xZ_y - k(x,y)\Big)^2} \\
&=& \expect{}{\Big(Z_y \eps_x + Z_x\eps_y + \eps_x \eps_y\Big)^2} +  \expect{}{\Big(Z_xZ_y - k(x,y)\Big)^2} \\
&=& \expect{}{Z_y^2 \eps_x^2 + Z_x^2\eps_y^2 + \eps_x^2 \eps_y^2} +  \sigma^2 \\
&\leq& k(y,y) \tsigma^2 + k(x,x) \tsigma^2 + \tsigma^4 +  \sigma^2 \\
&\leq& 2\tsigma^2 + \tsigma^4 +  \sigma^2. 
\end{eqnarray*}
\end{proof}

\begin{theorem}
\label{thm:varn}
For $x,y\in\cX$, assume we have random variables $Z_x,Z_y$ satisfying $\expect{}{Z_xZ_y} = k(x,y)$, and $\var{}{Z_x} = \var{}{Z_y} = \sigma^2$, and that $k(x,x) = k(y,y) = 1$.
Let $Q$ be any unbiased quantization function with bounded variance ($\expect{}{Q(z)} = z$, $\var{}{Q(z)} \leq \tsigma^2$ for any $z$).  Let $S=Z_x Z_y$, $T = Q(Z_x)Q(Z_y)$, and $(S_1,\ldots,S_n)$, $(T_1,\ldots,T_n)$ be a random sequence of i.i.d. draws from $S$ and $T$ respectively.  Define $\bar{S}_n = \frac{1}{n}\sum_{i=1}^n S_i$, and $\bar{T}_n =  \frac{1}{n}\sum_{i=1}^n T_i$, to be the empirical mean of these draws.  It follows that $\expect{}{\bS_n} = \expect{}{\bT_n} = k(x,y)$, and that
$\var{}{\bS_n} = \frac{\sigma^2}{n}$, and $\var{}{\bT_n} \leq \frac{2\tsigma^2 + \tsigma^4 +  \sigma^2}{n}$.
\end{theorem}
\begin{proof}
\begin{eqnarray*}
\var{}{\bS_n} &=& \var{}{\frac{1}{n}\sum_{i=1}^n S_i} \\
&=& \sum_{i=1}^n \var{}{\frac{1}{n}S_i} \\
&=& n \cdot \frac{\sigma^2}{n^2} \\
&=& \frac{\sigma^2}{n}
\end{eqnarray*}
The result for $\var{}{\bT_n}$ follows in the same way, using the result from Theorem \ref{lemma:var1}.
\end{proof}

I will now discuss a concentration bound for these low-precision random features:\\
\begin{theorem}
For a fixed $x,y\in\cX$, let $S$ be any random variable with the property that $\expect{}{S} = k(x,y)$, and $S \in [-2,2]$.  Let $(S_1,\ldots,S_n)$, be a random sequence of i.i.d. draws from $S$, and let $\bS_n = \frac{1}{n}\sum_{i=1}^n S_i$ be the empirical mean of this sequence.  Then, it follows directly from Hoeffding's inequality that $\Prob{\big[|\bS_n - k(x,y)| \geq \epsilon\big]} \leq 2\exp(-n\eps^2/8)$.
\end{theorem}

\noindent\textbf{Open Question}: Can Claim 1 (uniform convergence) from Rahimi and Recht [5] be adapted to this low-precision setting?

\subsubsection{Further analysis of variance}
In this section, we dig deeper into the bounds proven in \ref{thm:varn}, to understand whether under a fixed memory budget, we can attain lower kernel approximation error by using LP-RFF.
To do so, we will analyze the variance of using $\bS_n$ to approximate $k(x,y)$, relative to $\bT_{n*(32/b)}$.  This corresponds to comparing the variance of using $n$ ``full precision''  features (which we will assume are 32-bit), relative to using $n*(32/b)$ $b$-bit low-precision features. Note that both of these feature representations use the same total number of bits.  We will in this case assume we are using random Fourier features, and thus that
$z_x,z_y \in [-\sq,\sq]$.  We will upper bound $\sigma^2$ in this context by $1$, given the results in $[3]$ for the RBF kernel.  Furthermore, it is important to note that the quantization noise $\tsigma^2$ is very much tied to the number of bits used to quantize the features $z_x$.  We will thus use $\tsigma_b^2$ to denote the variance introduced by quantizing into $b$-bits.  In this case, we divide the interval $[-\sq,\sq]$ into $2^b-1$ sub-intervals of equal size, and quantization is performed within each of these intervals; each interval is of size $r = \frac{2\sq}{2^b-1}$. 
Thus, from Lemma \ref{lemma:qvar} the quantization error $\tsigma_b^2 \leq r^2/4 = \frac{2}{(2^b-1)^2}$.  This allows us to perform the comparison discussed above:

\begin{eqnarray*}
	\var{}{\bS_n} &=& \frac{\sigma^2}{n} \\
	&\leq& \frac{1}{n}. \\
	\var{}{\bT_{n*(32/b)}} &\leq& \frac{2\tsigma_b^2 + \tsigma_b^4 +  \sigma^2}{n*(32/b)} \\
	&\leq& \frac{2\frac{2}{(2^b-1)^2}  + \big(\frac{2}{(2^b-1)^2}\big)^2+ 1}{n*(32/b)}\\
	&\leq& \frac{\frac{4}{(2^b-1)^2} + \frac{4}{(2^b-1)^4} +  1}{n*(32/b)}\\
\end{eqnarray*}

We now plot these two upper bounds on a log-log plot, for various values of $b$ and $n$.\\
**OUTDATED**
\begin{center}
	\includegraphics[width=0.6\textwidth]{lprff_variance_figure.jpg}
\end{center}
As you can see, lowering the precision helps reduce the variance by approximately a full order of magnitude ($10\times$ smaller variance), with $b=2$ giving the lowest variance, and $b=32$ variance matching the full-precision line (they are overlapping in the figure).

\subsection{Spectrum of Low-Precision RFF}
\label{sec:spectrum}
In this section, we explain our empirical observation that the spectra of our kernel approximation matrices are much higher than the actual spectrum of the exact kernel matrix, in the case where we use 1, 2, or 4 bits to quantize our features.  If we consider the $n$ by $d$ RFF matrix $Z$, whose $i^{th}$ row is $z(x_i)$, and let $C$ denote the zero mean random quantization noise which is added to $Z$, it is easy to see that the kernel approximation matrix $(Z+C)(Z+C)^T$ has an elevated spetrum $\tlambda$, because:
\begin{eqnarray*} 
	\expect{}{\sum_i \tlambda_i} &=& \expect{}{trace[(Z+C)(Z+C)^T]} \\
	&=& \expect{}{trace[ZZ^T] + trace[ZC^T] + trace[CZ^T] + trace[CC^T]}\\
	&=& trace[ZZ^T] + \expect{}{trace[CC^T]} \\
	&>& trace[ZZ^T] \\
	&=& \expect{}{\sum_i \lambda_i}
\end{eqnarray*}
Notice that the spectrum $\tlambda$ of $(Z+C)(Z+C)^T$ relates to the spectrum $\tsigma$ of $(Z+C)$ via $\tlambda_i = \tsigma_i^2$.  Thus, letting $\sigma$ denote the spectrum of $Z$, we have shown that
$\expect{}{\sum_i \tsigma_i^2} > \sum_i \sigma_i^2$.  In other words, adding zero mean noise of any kind to a matrix elevates the $\ell_2$ norm of its spectrum.  This is intuitive because the noise will always increase the total weight on the diagonal of $ZZ^T$.
More interestingly, however, we can also show that if we take two random draws $C$ and $D$ of the quantization noise, the spectrum of $(Z+C)(Z+D)^T$ is also elevated relative to $ZZ^T$, even though the expected value of the entries of the diagonal of this matrix are \textit{equal} to the entries of the $ZZ^T$ diagonal.  This is because we can write $(Z+C)(Z+D)^T = ZZ^T + CZ^T + ZD^T + CD^T = ZZ^T + N$, where $N = CZ^T + ZD^T + CD^T$ is zero mean noise.  Thus, just like the spectrum of $Z+C$ was higher than that of $Z$ because we added zero mean noise $C$ to it, the spectrum of $(Z+C)(Z+D)^T$ will be higher than that of $ZZ^T$ because we are adding zero mean noise $N$ to it.  We show this in more detail below:

\begin{eqnarray*}
	A &=& ZZ^T \\
	&=& USU^T \\ %= \sum_i \lambda_i u_i u_i^T \\
	trace(A^T A) &=& trace( USU^T U S U^T)\\
	&=& trace( US^2 U^T) \\
	&=& \sum_i{\lambda_i^2}\\
	\tA &=& (Z+C)(Z+D)^T \\
	%	    &=& VTV^T \\
	\tA^T\tA &=& (Z+D)(Z+C)^T(Z+C)(Z+D)^T \\
	&=& ZZ^TZZ^T + ZC^TCZ^T + DZ^TZD^T + DC^TCD^T + \text{zero-mean}\\ 
	&=& A^TA + ZC^TCZ^T + DZ^TZD^T + DC^TCD^T + \text{zero-mean}\\ 
\end{eqnarray*}
\begin{eqnarray*}
	trace(\tA^T\tA) &=& trace(A^TA) + trace(ZC^TCZ^T) + trace(DZ^TZD^T) + trace(DC^TCD^T) + trace(\text{zero-mean})\\ 
	&=& \sum_i \tilde{\lambda_i}^2\\
	\expect{}{\sum_i \tilde{\lambda_i}^2} &=& \expect{}{trace(\tA^T\tA)} \\
	&=& trace(A^TA) + \expect{}{trace(ZC^TCZ^T) + trace(DZ^TZD^T) + trace(DC^TCD^T)} + \expect{}{trace(\text{zero-mean})}\\
	&=& trace(A^TA) + \expect{}{trace(ZC^TCZ^T) + trace(DZ^TZD^T) + trace(DC^TCD^T)}\\	
	&\geq& trace(A^TA) \\
	&=&  \sum_i{\lambda_i^2}. \\
	%\expect{}{z^T \tA^T\tA z} &=& z^TA^TAz + \expect{}{z^T(ZC^TCZ^T + DZ^TZD^T + DC^TCD^T)z} + %\expect{}{z^T\text{zero-mean}z}\\ 
	%&\geq& z^TA^TAz 
\end{eqnarray*}
%So taking $z = u_i$, we get $\|\tA u_i\|^2 \geq \|Au_i\|^2 = \lambda_i^2$.
%Can we show $\tA-A$ is PSD?

Now, we ask the following question: How large is the expected gap between $\expect{}{\sum_i \tilde{\lambda_i}^2}$ and $\sum_i{\lambda_i^2}$? To answer this, we consider the magnitude of 
$\expect{}{trace(ZC^TCZ^T) + trace(DZ^TZD^T) + trace(DC^TCD^T)}$, relative to the magnitude of
$\expect{}{trace[ZZ^TZZ^T]}$.  We begin by simply getting a simpler expression for $trace[AB^TBA^T]$ for
arbitrary matrices $A$ and $B$.\\

\noindent\textbf{Lemma}: $trace[AB^TBA^T] = \sum_{i,j=1}^n \sum_{k,\hk=1}^d a_{jk}a_{j\hk}b_{ik}b_{i\hk}$.\\

\noindent \textbf{Proposition}:  $\expect{}{trace[ZZ^TZZ^T]} = n + \frac{n^2-n}{d}$, while 
$\expect{}{trace(ZC^TCZ^T) + trace(DZ^TZD^T) + trace(DC^TCD^T)} = \frac{4n^2}{d(2^b-1)^2} + \frac{4n^2}{d(2^b-1)^4} = \frac{4n^2}{d}\Big(\frac{1}{(2^b-1)^2} + \frac{1}{(2^b-1)^4}\Big)$. \\

\noindent \textit{Proof}:  We will use the above Lemma to compute expected values for $trace[ZZ^TZZ^T]$, $trace(ZC^TCZ^T)$, $trace(DZ^TZD^T)$, and $trace(DC^TCD^T)$.  This will directly prove the proposition.

\begin{itemize}
	\item Case 1: $A=B=Z$.
	\begin{eqnarray*}
		\expect{}{trace[ZZ^TZZ^T]} &=& \sum_{i,j=1}^n \sum_{k,\hk=1}^d \expect{}{z_{jk}z_{j\hk}z_{ik}z_{i\hk}}\\
		&=& \sum_{i=1}^n \sum_{k,\hk=1}^d \expect{}{z_{ik}^2}\expect{}{z_{i\hk}^2} + \sum_{i\neq j} \sum_{k=1}^d \expect{}{z_{jk}^2}\expect{}{z_{ik}^2}\\
		&=& \sum_{i=1}^n \sum_{k,\hk=1}^d \frac{1}{d^2} + \sum_{i\neq j} \sum_{k=1}^d \frac{1}{d^2}\\
		&=& nd^2\frac{1}{d^2} + (n^2-n)d\frac{1}{d^2}\\
		&=& n + \frac{n^2-n}{d}.
	\end{eqnarray*}
	Above, we used the fact that $\expect{}{z_{ik}^2} = \expect{w_k,b_k}{\frac{2}{d}\cos(w_k^T x_i + b_k)^2} = \frac{k(x_i,x_i)}{d} = \frac{1}{d}$.
	\item Case 2: $A=Z$, $B=C$ (or $A=D$ and $B=Z$).
	\begin{eqnarray*}
		\expect{}{trace[ZC^TCZ^T]} &=& \sum_{i,j=1}^n \sum_{k,\hk=1}^d \expect{}{z_{jk}z_{j\hk}c_{ik}c_{i\hk}}\\	
		&=& \sum_{i,j=1}^n \sum_{k=1}^d \expect{}{z_{jk}^2}\expect{}{c_{ik}^2}\\	
		&\leq& \sum_{i,j=1}^n \sum_{k=1}^d \frac{1}{d}\cdot \frac{1}{4}\bigg(\frac{2\sqrt{2/d}}{2^b-1}\bigg)^2\\	
		&=& n^2d \cdot \frac{1}{d}\cdot \frac{1}{4}\cdot \frac{8}{d(2^b-1)^2}\\	
		&=& \frac{2n^2}{d(2^b-1)^2} \\
		&=& O\Big(\frac{n^2}{d\cdot 2^{2b}}\Big).
		%&\approx& \frac{2n^2}{d2^{2b}} \\
		%&=& \frac{n^2}{d}\Big(2^{-2b+1}\Big) \\
	\end{eqnarray*}
	Above, we use the fact that $\expect{}{c_{ik}^2} \leq \frac{1}{4}\bigg(\frac{2\sqrt{2/d}}{2^b-1}\bigg)^2 = \frac{2}{d(2^b-1)^2}$, and that $\expect{}{z_{ik}^2} = \frac{1}{d}$.  Note that this bound also holds when $A=D$ and $B=Z$.
	\item Case 3: $A=C$, $B=D$.
	\begin{eqnarray*}
		\expect{}{trace[CD^TDC^T]} &=& \sum_{i,j=1}^n \sum_{k,\hk=1}^d \expect{}{c_{jk}c_{j\hk}d_{ik}d_{i\hk}}\\
		&=& \sum_{i,j=1}^n \sum_{k=1}^d \expect{}{c_{jk}^2}\expect{}{d_{ik}^2}\\
		&=& \sum_{i,j=1}^n \sum_{k=1}^d \bigg(\frac{2}{d(2^b-1)^2}\bigg)^2\\
		&=& n^2d \cdot\frac{4}{d^2(2^b-1)^4}\\
		&=& \frac{4n^2}{d(2^b-1)^4}\\
		&=& O\Big(\frac{n^2}{d \cdot 2^{4b}}\Big).
	\end{eqnarray*}
\end{itemize}
This concludes the proof.

\section{Generalization Performance of Low-Precision RFF}
\label{sec:gen}

\begin{theorem} (extending Theorem 1 of $[4]$ to LP-RFF): \\
\label{thm:gen}
Let $\phi:\cX\times\Omega\rightarrow\RR$ be a set of feature functions such that $\sup_{x,w}|\phi(x;w)| \leq 1$. Assume there exists a family of deterministic quantization functions $\{Q_{\theta}:\RR\rightarrow\RR \;|\; \theta\in\Theta \}$, parameterized by some parameter $\theta\in\Theta$.  Let $p_{\Theta}$ be a probability distribution over $\Theta$, $p_{\Omega}$ be a probability distribution over $\Omega$, and define $p(w,\theta) = p_{\Omega}(w)p_{\Theta}(\theta)$.
Define the quantized feature functions as $\phi'(x;w,\theta) = Q_{\theta}(\phi(x;w))$, and define
$$\cF_p = \bigg\{f(x) = \int_{\Omega\times\Theta} \alpha(w,\theta) \phi'(x;w,\theta)dwd\theta \;\;\bigg|\;\; |\alpha(w,\theta)| \leq Cp(w,\theta)\bigg\}.$$
Suppose the cost function $c(y,y') = c(yy')$, with $c(yy')$ $L$-Lipschitz.  Then for any $\delta > 0$, if the training data $\{x_i,y_i\}_{i=1}^n$ are drawn iid from some distribution $P$, Algorithm \ref{alg:lprff}  returns a function $\hat{f}$ that satisfies:
$$R[\hat{f}] - \min_{f\in\cF_p}R[f] \leq O\Bigg(\bigg(\frac{1}{\sqrt{n}} + \frac{1}{\sqrt{m}}\bigg)LC \sqrt{\log\frac{1}{\delta}}\Bigg)$$
with probability at least $1-2\delta$ over the training dataset and the choice of the parameters $w_i,\theta_i$.
(Here, $R[F] = \expect{(x,y)\sim P}{c(f(x),y)}$)
%with the property that $\expect{\theta}{Q_{\theta}(z)} = z$, where this expectation is taken with respect to some probability distribution over $\theta$.
\end{theorem}

\begin{algorithm}
\caption{%
	Low-Precision Weighted Sum of Random Kitchen Sinks Training (adapted from $[4]$)%
}
\label{alg:lprff}
\begin{algorithmic}[1]
	\renewcommand{\algorithmicrequire}{\textbf{Input}}
	\REQUIRE
	A dataset $\{x_i,y_i\}_{i=1}^n$ of $n$ points, a family of bounded feature functions $|\phi(x;w)|\leq 1$ parameterized by $w\in\Omega$, a family of bounded quantization functions $|Q(z;\theta)| \leq 1$ parameterized by $\theta \in \Theta$, an integer $m$, a scalar $C$, probability distributions  $p_{\Omega}$ and $p_{\Theta}$ over $\Omega$ and $\Theta$.
	\renewcommand{\algorithmicrequire}{\textbf{Output}}
	\REQUIRE
	A function $\hat{f}(x) = \sum_{i=1}^m Q(\phi(x;w_i);\theta_i) \alpha^*_i$.
	\STATE Draw $w_1,\ldots,w_m$ from $p_{\Omega}$, and $\theta_1,\ldots,\theta_m$ from $p_{\Theta}$.
	\STATE Featurize the input: $z_i \leftarrow [Q(\phi(x_i;w_1);\theta_1),\ldots,Q(\phi(x_i;w_m);\theta_m)]^T$.
	\STATE With the $w_i$ and $\theta_i$ fixed, solve the empirical risk minimization problem:
	\begin{eqnarray*}
		\alpha* =\argmin_{\alpha\in\RR^m} \frac{1}{n}\sum_{i=1}^n c(\alpha^T z_i,y_i)\\
		s.t. \|\alpha\|_\infty < C/m
	\end{eqnarray*}
	\RETURN $\hat{f}(x) = \sum_{i=1}^m Q(\phi(x;w_i);\theta_i) \alpha^*_i$.
\end{algorithmic}
\end{algorithm}


\noindent \textbf{Open Question}: How does the family of functions $\cF_p$ defined in Proposition 3 compare to the one in $[4]$ (Rahimi and Recht, 2008)?	

\subsection{``Deterministic'' Quantization Functions}
In Theorem \ref{thm:gen}, we defined a set of deterministic quantization functions
$\{Q_{\theta}:\RR\rightarrow\RR \;|\; \theta\in\Theta \}$, parameterized by some parameter $\theta\in\Theta$.
We now discuss how to construct these.  The idea is simple: For every quantization interval $[a,c]$, we choose a threshold value $t \in [a,c]$ uniformly at random.  Then if a certain unquantized feature $z$ falls in this interval, we will set $Q_t(z)= a$ if $z \leq t$, and $Q_t(z) = c$ if $z > t$.  It is easy to show that $\expect{t}{Q_t(z)} = z$:
\begin{eqnarray*}
	\expect{t}{Q_t(z)} &=& a \cdot \Prob[z \leq t] + c \cdot \Prob[z > t]  \\
	&=& a \cdot \frac{c-z}{c-a} + c\cdot \frac{z-a}{c-a} \\
	&=& \frac{ac - az + cz - ac}{c-a} \\
	&=& z.
\end{eqnarray*}
Thus, if for every quantization interval we draw a random threshold $t_i$ as described, and concatenate these thresholds as $\theta = (t_1,\ldots,t_{2^b-1})$, we have successfully constructed a set of unbiased and deterministic quantization functions, as desired.

\subsection{Effect of quantization noise at test time}
In this section, we show that the quantization noise at test time results in larger expected 
mean-squared error, in the context of kernel ridge regression.

\begin{theorem}
\label{thm:testvar}
Let $Q:\RR^m\rightarrow\RR^m$ be an unbiased (element-wise) random quantization function with bounded variance ($\var{}{Q(z)_i} \leq \tsigma^2 \; \forall z,i$), and let $f_w(z) = w^T z$ be a fixed regression model.  It follows that the expected mean-squared error, using quantized features $Q(z(x))$, satisfies $$\expect{}{\Big(f_w(Q(z(x))) - y\Big)^2} \leq \expect{}{\Big(f_w(z(x)) - y\Big)^2} + \|w\|^2\tsigma^2.$$
\end{theorem}

\begin{proof}
Let $z(x)$ denote the full precision RFF representation for $x$, and $Q(z(x)) = z(x) + \eps_x$ represents the randomly quantized representation.
\begin{eqnarray*}
	\expect{}{\Big(f_w(Q(z(x))) - y\Big)^2} &=& \expect{}{(w^T (z(x) + \eps_x) -y)^2} \\
	&=& \expect{}{(w^T z(x) + w^T \eps_x -y)^2} \\
	&=& \expect{}{(w^T z(x) - y)^2)} + \expect{}{(w^T \eps_x)^2} \\
	&=&  \expect{}{\Big(f_w(z(x)) - y\Big)^2} + \expect{}{\Big(\sum_i w_i \eps_{x,i}\Big)^2} \\
	&=&  \expect{}{\Big(f_w(z(x)) - y\Big)^2}+ \sum_i w_i^2 \expect{}{\eps_{x,i}^2} \\
	&=&  \expect{}{\Big(f_w(z(x)) - y\Big)^2}  + \|w\|^2\expect{}{\eps_{x,1}^2} \\
	&\leq&  \expect{}{\Big(f_w(z(x)) - y\Big)^2} + \|w\|^2\tsigma^2.
%	&\leq& \expect{}{(w^T z(x) - y)^2)} + \|w\|^2\cdot\frac{1}{4}\bigg(\frac{2\sqrt{2/d}}{2^b-1}\bigg)^2\\
%	&=& \expect{}{(w^T z(x) - y)^2)} + \|w\|^2\cdot\frac{2}{d(2^b-1)^2}.
\end{eqnarray*}
\end{proof}
This theorem shows that our expected error on a test point $(x,y)$, if we use quantized features at test time, is in expectation $\|w\|^2\tsigma^2$ larger than the expected error if we used the full precision features at test time.  In the case where $Q$ quantizes each feature into $b$ bits, $\tsigma^2= \frac{2}{m(2^b-1)^2}$.
This suggests that it could be desirable, in certain contexts, to train using low precision (to speed up training), but use full precision features at test time.


\subsection{Fixed Design Kernel Ridge Regression}
Discuss results from other notes.

%We now discuss how to apply the result from Theorem 1 of Rahimi and Recht's 2008 paper to this low-precision setting [4].  That theorem assumes that there is a set of (deterministic) basis functions $\phi: \cX \times \Omega \rightarrow \RR$.  It then argues that if $\{w_1,\ldots,w_K\}$ are drawn independently from some distribution $p$ on $\Omega$, then the generalization performance of the model trained on the features $\phi(\;\cdot\;;w_i)$ is close to the best possible generalization performance of any model in the set $\cF_p = \{f(x) = \int_\Omega \alpha(w)\phi(x;w)dw | |\alpha(x)| \leq Cp(w)\}$.  Unfortunately, our randomly quantized features currently do not fit nicely into this framework, because the basis functions themselves are random, even for a fixed $x$ and $w$.  The purpose of the section below is to construct a parametrized set of basis functions $\phi(x;w,b,\theta)$ which quantize $\cos(w^Tx+b)$ in a deterministic way, given the parameters $\theta$.  



\section{Training a model using low-precision random features}
\label{sec:train}
In this section, we address the question of how to train a model using low-precision random features as input.  Essentially, this comes down to training a linear model on top of random features $\tz$ such that $\expect{}{\tz} = z$.  One potentially
great option is to use ``LM-HALP'', the version of HALP designed for learning linear models, presented as Algorithm 4 in the
recent submission.  One potential down-side to this approach is that it would mean that the learned model would be
full-precision, which may or may not be desirable (for example, computing the full-precision gradient over the entire dataset
could be prohibitively expensive, and perhaps even impossible in the case where the full-precision model doesn't fit in
memory).  Note that any of the options below which use low-precision for both train/test would be inherently limited in terms of
how close to the global optimum they could get, as discussed in the HALP paper.  Perhaps, however, the error introduced by this
quantization could be more than made-up for by the ability to learn a model in a higher-dimensional, more expressive, feature
space.  We present some alternative options to LM-HALP below:
\begin{itemize}
	\item \textbf{Perceptron updates}: We could use the perceptron algorithm for model updates, 
	which would ensure that all operations could be done in integer arithmetic.
	\item\textbf{ Quantized SGD updates (``LP-SGD'')}: Consider the full-precision SGD update of
	the form $w_{t+1} = w_t + g_t z_t$, where $z_t = z(x_t) \in \RR^m$ is the random
	feature representation corresponding to the randomly chosen training point $x_t$ at
	time $t$.  We could replace this update by updates of the form 
	$w_{t+1} = w_t + \tg_t \tz_t$, where $\expect{}{\tg_t} = g_t$, and $\tg_t$ is stored in
	low-precision format.  For example, in the case of logistic regression, 
	$g_t = \eta \big(y_t - p_t\big)$, where $y_t\in\{0,1\}$ is the label for $x_t$, $\eta \in \RR$ is the learning
	rate, and $p_t = p(Y_t=1|z_t,w_t) = (1+\exp(-w_t^Tz_t))^{-1}$.  So in the case where $y_t = 1$,
	$\tg_t$ could be equal to $1$ with probably $1-p_t$, and 0 otherwise; and in the case
	where $y_t = 0$, $\tg_t$ could be equal to $-1$ with probability $p_t$, and 0
	otherwise.  Here, I am assuming that $\eta$ is stored in the scale factor $\delta$
	of the low-precision format $(\delta,b)$ for $\tg_t$.	
	If $\tg_t$ is in $(\delta,b)$ low-precision format, and $\tz_t$ is in
	$(\delta',b')$ format, then $w_t$ would be in $(\delta\delta',b+b')$ format.
	Note also that computing $p_t$ requires using the $\exp(\cdot)$
	and $(\cdot)^{-1}$ operations, which would probably need to be done as floating point
	operations.
	\item \textbf{LP-SVRG}: We could directly use LP-SVRG.
	\item \textbf{Update dual variables instead of primal}: In terms of the dual variables $\alpha_i$, the
	model becomes $f(x) = \sum_{i=1}^n \alpha_i z(x_i)^T z(x)$. Letting 
	$Z\in \RR^{n\times m}$ be the matrix whose $i^{th}$ row is $z(x_i)$, this can be
	rewritten as $\alpha^T (Z z(x))$.  If the random features are binary, $Zz(x)$ 
	can be implemented very efficiently, and its output is an integer vector (note that for huge $Z$,
	this operation can be distributed across a cluster of machines).
	If $\alpha$ is stored in low-precision format (as it would be if updates of the form described above
	for perceptron or LP-SGD are used), this dot-product could be performed using low-precision
	integer arithmetic, which can also be implemented fast.  As far as updating the values of
	the dual parameters $\alpha_i$ during training, we can simply simulate the primal updates of the form 
	$w_{t+1} = w_t + \tg_t z_t$ by using $\alpha_t = \alpha_t + \tg_t$, where here I am using 
	$\alpha_t$ to denote the dual parameter corresponding to the random point $x_t$ chosen at time $t$.
	\item \textbf{Distributed optimization}: We could also consider large-scale distribution optimization algorithms (\eg, $[1,2]$) in order to speed up training.
\end{itemize}


\subsection{Computing the low-precision random features}
In this entire document so far, we have considered the setting where we compute quantized features by first computing the full precision features, and then performing random quantization.  Another option is to 
first quantize $x$, $W$, and $b$, as $\tx$, $\tW$, $\tb$.  Then, compute $\cos(\tW^T \tx + \tb)$, and quantize the output.
One important thing to note is that quantizing $x$,$W$, and $b$ in such a way that $\expect{\tx,\tW,\tb}{\tW^T\tx + \tb} = W^Tx+b$ does \textit{not} mean that $\expect{\tx,\tW,\tb}{\cos(\tW^T\tx + \tb)} = \cos(W^Tx + b)$.  Thus, features generated in this way would not necessarily produce unbiased estimates of the kernel function, which satisfy $\expect{}{z(x)^Tz(y)} = k(x,y)$.

\noindent \textbf{Open Question}: Is there anything we can prove about computing low-precision features in the above manner? Would they perform well empirically?
\section{Experiments}
\label{sec:experiments}
\input{experiments}

%(z_x+\teps_x)(z_y+\teps_y)$, where $\teps_x \sim X_{z_x}^{-\sq,\sq}$, and $\teps_y \sim X_{z_y}^{-\sq,\sq}$.
%Then $\var{}{\tS} = 4 - z_x^2 z_y^2$.
\section{References}
\noindent$[1]$ Large Scale Kernel Learning using Block Coordinate Descent.
Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, Benjamin Recht. \url{https://arxiv.org/pdf/1602.05310.pdf}. \\
$[2]$ CoCoA: A General Framework for Communication-Efficient Distributed Optimization.
Virginia Smith, Simone Forte, Chenxin Ma, Martin Takac, Michael I. Jordan, Martin Jaggi.  \url{https://arxiv.org/abs/1611.02189} \\
$[3]$ On the Error of Random Fourier Features. Dougal J. Sutherland, Jeff Schneider. UAI 2015. \url{https://arxiv.org/abs/1506.02785}\\
$[4]$ Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning. Ali Rahimi, Ben Recht. NIPS 2008.\\
$[5]$	Random Features for Large-Scale Kernel Machines. Ali Rahimi, Benjamin Recht. NIPS 2007:
\end{document}
