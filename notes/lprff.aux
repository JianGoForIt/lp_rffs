\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Kernel Approximation Error for LP-RFF}{2}{section.2}}
\newlabel{sec:kernel_approx}{{2}{2}{Kernel Approximation Error for LP-RFF}{section.2}{}}
\newlabel{sec:kernel_approx@cref}{{[section][2][]2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Kernel Approximation Variance Analysis}{2}{subsection.2.1}}
\newlabel{lemma:qvar}{{2.1}{2}{}{theorem.2.1}{}}
\newlabel{lemma:qvar@cref}{{[theorem][1][2]2.1}{2}}
\newlabel{lemma:var1}{{2.2}{2}{}{theorem.2.2}{}}
\newlabel{lemma:var1@cref}{{[theorem][2][2]2.2}{2}}
\newlabel{thm:varn}{{2.3}{3}{}{theorem.2.3}{}}
\newlabel{thm:varn@cref}{{[theorem][3][2]2.3}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Further analysis of variance}{4}{subsubsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Kernel Approximation Variance, as a function of the number of features used (a), and the amount of memory used (b). Note that in the left plot, the 4-bit, 8-bit, 16-bit, and full-precision lines all overlap.}}{5}{figure.1}}
\newlabel{fig:kernel_approx_var}{{1}{5}{Kernel Approximation Variance, as a function of the number of features used (a), and the amount of memory used (b). Note that in the left plot, the 4-bit, 8-bit, 16-bit, and full-precision lines all overlap}{figure.1}{}}
\newlabel{fig:kernel_approx_var@cref}{{[figure][1][]1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Spectrum of Low-Precision RFF}{5}{subsection.2.2}}
\newlabel{sec:spectrum}{{2.2}{5}{Spectrum of Low-Precision RFF}{subsection.2.2}{}}
\newlabel{sec:spectrum@cref}{{[subsection][2][2]2.2}{5}}
\newlabel{prop:traceNoise}{{2.5}{6}{}{theorem.2.5}{}}
\newlabel{prop:traceNoise@cref}{{[theorem][5][2]2.5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Generalization Performance of Low-Precision RFF}{9}{section.3}}
\newlabel{sec:gen}{{3}{9}{Generalization Performance of Low-Precision RFF}{section.3}{}}
\newlabel{sec:gen@cref}{{[section][3][]3}{9}}
\newlabel{thm:gen}{{3.1}{9}{}{theorem.3.1}{}}
\newlabel{thm:gen@cref}{{[theorem][1][3]3.1}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Low-Precision Weighted Sum of Random Kitchen Sinks Training (adapted from $[4]$)}}{9}{algorithm.1}}
\newlabel{alg:lprff}{{1}{9}{Generalization Performance of Low-Precision RFF}{algorithm.1}{}}
\newlabel{alg:lprff@cref}{{[algorithm][1][]1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}``Deterministic'' Quantization Functions}{9}{subsection.3.1}}
\newlabel{sec:deterministic}{{3.1}{9}{``Deterministic'' Quantization Functions}{subsection.3.1}{}}
\newlabel{sec:deterministic@cref}{{[subsection][1][3]3.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Effect of quantization noise at test time}{10}{subsection.3.2}}
\newlabel{sec:reduce_test_noise_theory}{{3.2}{10}{Effect of quantization noise at test time}{subsection.3.2}{}}
\newlabel{sec:reduce_test_noise_theory@cref}{{[subsection][2][3]3.2}{10}}
\newlabel{thm:testvar}{{3.2}{10}{}{theorem.3.2}{}}
\newlabel{thm:testvar@cref}{{[theorem][2][3]3.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fixed Design Kernel Ridge Regression}{11}{subsection.3.3}}
\newlabel{sec:fixed_design}{{3.3}{11}{Fixed Design Kernel Ridge Regression}{subsection.3.3}{}}
\newlabel{sec:fixed_design@cref}{{[subsection][3][3]3.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}When is it safe to use low precision?}{12}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Generalizatio bound using $\epsilon $-log-spectral distance}{13}{subsection.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Training a model using low-precision random features}{15}{section.4}}
\newlabel{sec:train}{{4}{15}{Training a model using low-precision random features}{section.4}{}}
\newlabel{sec:train@cref}{{[section][4][]4}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Computing the low-precision random features}{16}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Kernel approximation and validation L2 loss for Kernel Ridge Regression on UCI Census dataset. (a) and (c) compare different precision representation with the same number of random Fourier features. (b) and (d) compare different precision representation under same memory bits budgets.}}{17}{figure.2}}
\newlabel{fig:kernel_and_l2}{{2}{17}{Kernel approximation and validation L2 loss for Kernel Ridge Regression on UCI Census dataset. (a) and (c) compare different precision representation with the same number of random Fourier features. (b) and (d) compare different precision representation under same memory bits budgets}{figure.2}{}}
\newlabel{fig:kernel_and_l2@cref}{{[figure][2][]2}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{17}{section.5}}
\newlabel{sec:experiments}{{5}{17}{Experiments}{section.5}{}}
\newlabel{sec:experiments@cref}{{[section][5][]5}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Kernel Ridge Regression}{17}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Spectrum (eigen values) of the kernel matrix. (a) The spectrums from different precision representation is shown under 8192 random Fourier features. (b) Configurations with similar approximation errors can demonstrate very different spectrum; we compare the 1 bit representation to the full precision configurations giving 1) the highest approx. error that is lower than the 1 bit configuration 2) the lowest approx. error that is higher than the 1bit configuration. (c) The histogram of per-entry kernel approximation error; the histogram is aggregated from 10 runs with different random seed.}}{18}{figure.3}}
\newlabel{fig:spectrums}{{3}{18}{Spectrum (eigen values) of the kernel matrix. (a) The spectrums from different precision representation is shown under 8192 random Fourier features. (b) Configurations with similar approximation errors can demonstrate very different spectrum; we compare the 1 bit representation to the full precision configurations giving 1) the highest approx. error that is lower than the 1 bit configuration 2) the lowest approx. error that is higher than the 1bit configuration. (c) The histogram of per-entry kernel approximation error; the histogram is aggregated from 10 runs with different random seed}{figure.3}{}}
\newlabel{fig:spectrums@cref}{{[figure][3][]3}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Kernel approximation error and validation L2 loss}{18}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Spectrum of kernel matrix correlates with L2 loss}{18}{subsubsection.5.1.2}}
\newlabel{subsubsec:bump_spectrum}{{5.1.2}{18}{Spectrum of kernel matrix correlates with L2 loss}{subsubsection.5.1.2}{}}
\newlabel{subsubsec:bump_spectrum@cref}{{[subsubsection][2][5,1]5.1.2}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The kernel approximation error and regression performance (validation L2 loss) from the 4 models in Figure\nobreakspace  {}\ref  {fig:spectrums} (b). Similar kernel approximation errors do not always imply similar regression performance.}}{19}{table.1}}
\newlabel{tab:cor_approx_err_l2}{{1}{19}{The kernel approximation error and regression performance (validation L2 loss) from the 4 models in Figure~\ref {fig:spectrums} (b). Similar kernel approximation errors do not always imply similar regression performance}{table.1}{}}
\newlabel{tab:cor_approx_err_l2@cref}{{[table][1][]1}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Effect of independent quantization of RFF matrix and its transpose}{19}{subsubsection.5.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Effect of reducing variance in test sample features}{20}{subsubsection.5.1.4}}
\newlabel{subsubsec:test_var_reduce}{{5.1.4}{20}{Effect of reducing variance in test sample features}{subsubsection.5.1.4}{}}
\newlabel{subsubsec:test_var_reduce@cref}{{[subsubsection][4][5,1]5.1.4}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Future Directions}{20}{section.6}}
\newlabel{sec:future}{{6}{20}{Future Directions}{section.6}{}}
\newlabel{sec:future@cref}{{[section][6][]6}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Spectrum (eigen values) of the kernel matrix. (a) The spectrum from different precision representation under 32k bits memory budget (1024 full precision rffs) for the feature of each sample. (b) The spectrum from different precision representation under 25.6k bits memory budget (1024 full precision rffs) for the feature of each sample. (c) and (d) is re-visualization of (a) and (b) in log-scale; it zooms in small value regions.}}{21}{figure.4}}
\newlabel{fig:indep_quant}{{4}{21}{Spectrum (eigen values) of the kernel matrix. (a) The spectrum from different precision representation under 32k bits memory budget (1024 full precision rffs) for the feature of each sample. (b) The spectrum from different precision representation under 25.6k bits memory budget (1024 full precision rffs) for the feature of each sample. (c) and (d) is re-visualization of (a) and (b) in log-scale; it zooms in small value regions}{figure.4}{}}
\newlabel{fig:indep_quant@cref}{{[figure][4][]4}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces We train with low precision RFF and test with full precision RFF. Though the model is trained with low precision features, the test L2 loss can be improved by reducing variance in test RFF. (a) compare different precision representation under same memory bits budgets. (b) compare different precision representation with the same number of random Fourier features. }}{21}{figure.5}}
\newlabel{fig:var_reduction}{{5}{21}{We train with low precision RFF and test with full precision RFF. Though the model is trained with low precision features, the test L2 loss can be improved by reducing variance in test RFF. (a) compare different precision representation under same memory bits budgets. (b) compare different precision representation with the same number of random Fourier features}{figure.5}{}}
\newlabel{fig:var_reduction@cref}{{[figure][5][]5}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {7}References}{22}{section.7}}
