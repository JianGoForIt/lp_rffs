\subsection{Notation}
We use $\{(x_i,y_i)\}_{i=1}^n$ to denote a training set, for $x_i \in \RR^d$, and $y_i \in \cY$, where $\cY = \RR$ for regression, and $\cY = \{1,\ldots,c\}$ for classification.  We let $K$ denote the kernel matrix corresponding to a kernel function $k:\RR^d\times\RR^d$, where $K_{ij} = k(x_i,x_j)$, and let $\tK$ denote an approximation to $K$. We let $z:\RR^d\rightarrow\RR^m$ denote a feature map for approximation a kernel function, such that $\tK_{ij} = z(x_i)^T z(x_j)$.  We use $s$ to denote the size of the mini-batches during training, $b$ to denote the precision used for the random features, and $\hb$ to denote the precision used for the model.  We let $\|K\|_2$ and $\|K\|_F$ denote the spectral and Frobenius norms of a matrix $K$, respectively; if the subscript is not specified, $\|K\|$ denotes the spectral norm.

\subsection{Kernel Approximation Methods}
We now review the \Nystrom method and random Fourier features, two of the most widely used and studied methods for kernel approximation.

\paragraph{Random Fourier features (RFFs)}
For shift-invariant kernels ($k(x,x') = \hat{k}(x-x')$), the random Fourier 
feature method \citep{rahimi07random} constructs a random feature representation 
$z(x) \in \RR^m$ such that $\expect{}{z(x)^T z(x')} = k(x,x')$. This construction 
is based on Bochner's Theorem, which states that any positive definite kernels is 
equal to the Fourier transform of a non-negative measure. This allows for performing
Monte Carlo approximations of this Fourier transform in order to approximate the 
function.  The resulting features have the following functional form: 
$z_i(x) = \sqrt{2/m}\cos(w_i^Tx + a_i)$, where $w_i$ is drawn from the inverse Fourier
transform of the kernel function $\hat{k}$, and $a_i$ is drawn uniformly from $[0,2\pi]$. 

%One way of reducing the memory required for storing $W=[w_1;\ldots; w_m]$, is to replace $W$ by a structured matrix; in this work, we let $W$ be a concatenation of many square circulant random matrices (circ-RFFs) \citep{yu15}.

\paragraph{\Nystrom method}
The \Nystrom method constructs a finite dimensional feature representation
$z(x) \in \RR^m$ such that $\Dotp{z(x),z(x')} \approx k(x,x')$.  It does this
by picking a set of landmark points $\{\hx_1,\ldots,\hx_m\} \in \cX$,
and taking the SVD $\hK = U\Lambda U^T$ of the $m$ by $m$ 
kernel matrix $\hK$ corresponding to these landmark points 
($\hK_{i,j} = k(\hx_i,\hx_j)$).  The \Nystrom representation for a point $x \in \cX$
is defined as $z(x) = \Lambda^{-1/2} U^T k_x$, where $k_x = [k(x,\hx_1),\ldots,k(x,\hx_m)]^T$.
Letting $K_{m,n} = [k_{x_1},\ldots,k_{x_n}] \in \RR^{m\times n}$, 
the \Nystrom method can be thought of as an efficient low-rank approximation
$K \approx K_{m,n}^T U \Lambda^{-1/2}\Lambda^{-1/2} U^T K_{m,n}$ of the full
$n$ by $n$ kernel matrix $K$ corresponding to the full dataset $\{x_i\}_{i=1}^n$.
One can also consider the lower dimension \Nystrom representation
$z_r(x) = \Lambda_r^{-1/2} U_r^T k_x \in \RR^r$, where only the top $r$ eigenvalues and
eigenvectors of $\hK$ are used, instead of all $m$.

\subsection{Relative spectral distance vs. $\Delta$-spectral approximation}
\citet{avron17} define $\tK+\lambda I$ to be a $\Delta$-spectral approximation of $K+\lambda I$ if $(1-\Delta)(K+\lambda I) \preceq \tK + \lambda I \preceq (1+\Delta)(K+\lambda I)$. This differs from our definition in two important ways. First, we define the distance as the \textit{minimum} value of $\Delta$ satisfying the bound.  Second of all, we use $(1+\Delta)^{-1}(K+\lambda I)$ in place of $(1-\Delta)(K+\lambda I)$. The definition using $(1-\Delta)$ has the property that the maximum value of $\Delta$ for the ``left inequality'' is 1, while the maximum value is $+\infty$ for the ``right inequality.'' Empirically, this asymmetry results in $\Delta$ only correlating strongly with generalization performance for small values of $\Delta$, at which $1-\Delta\approx (1+\Delta)^{-1}$. Our definition, on the other hand, correlates strongly for all $\Delta$, as we show in Section \ref{sec:experiments}.  From a theoretical perspective, our definition also allows us to prove generalization bounds (Proposition \ref{prop:avron}) which hold for all $\Delta \geq 0$, whereas the previous results on hold for $\Delta < 1$.


\subsection{Generalization bound in terms of relative spectral distance}
\begin{proposition}{Adapted from \citep{avron17}:}
	Suppose $\tK$ is an approximation to a kernel matrix $K$, and $f_{K}$ and $f_{\tK}$ are the KRR estimators learned using these matrices, with regularizing constant $\lambda$ and label noise variance $\sigma^2$. Then the following bound holds:
	\begin{eqnarray}
	\cR(f_{\tK}) \leq \Big(1+D_{\lambda}(K,\tK)\Big)\cdot \hcR(f_K) + \frac{D_{\lambda}(K,\tK)}{1+D_{\lambda}(K,\tK)}\cdot \frac{rank(\tK)}{n}\cdot\sigma^2.
	\end{eqnarray}
\end{proposition}
\begin{proof}
	The proof exactly follows the proof of \citep{avron17}, with the only differences being that we (1) replace every appearance of $(1-\Delta)^{-1}$ with $(1+D_{\lambda}(K,\tK))$, and (2) replace every appearance of $(1+\Delta)$ with $(1+D_{\lambda}(K,\tK))$.  This is a a result of the difference between the way we define relative spectral distance, and how they define $\Delta$-spectral approximation.
\end{proof}

\subsection{Related work}
Our work is not the first to attempt to minimize the memory footprint of kernel approximation methods.  One line of work proposes using structured matrices in place of the projection matrix used by random Fourier features \citep{fastfood,yu15,sphereRKS}; these methods generally reduce the storage requirement for the projection matrix from $O(md)$ to $O(m)$, where $d$ is the dimension of the data. This work is orthogonal to ours, and in fact we use circulant projections \citep{yu15} in our implementation of LP-RFFs; importantly, these methods do not reduce the amount of memory occupied by the features themselves, or by the learned model parameters. Another line of work uses feature selection in order to reduce the number of random Fourier features required to attain a specific level of performance \citep{sparseRKS, may2016}; once again, our work is orthogonal to theirs, because feature selection can be performed on the low-precision features.  For the \Nystrom method, there has been extensive work on the best ways of choosing the landmark points \citep{kmeans08,kumar12,gittens13}.  Several other methods reduce the memory requirement for \Nystrom \cite{ensemble09,fastpred14,meka14}. In this paper, however, we focus our efforts on random Fourier features, which we observe perform significantly better than the \Nystrom method, under a fixed memory budget.

On the topic of scaling kernel methods to large datasets, there have been a number of notable recent papers.  From an algorithmic perspective, \citet{block16} propose a distributed block coordinate descent method for solving large-scale least squares problems using the \Nystrom method or random Fourier features. Although they focus on the distributed setting (1024 cores on 128 machines) and we do not, our contribution could nonetheless help lower the amount of memory and computation performed by each worker.  The recent work of \citet{may2017} uses a single GPU to train large RFF models for speech recognition, on datasets with up to 16 million training points and 5000 classes, showing comparable performance to deep neural networks. In that work they were limited by the number of features they could fit on a single GPU, and thus our work could also help them scale their experiments to even larger numbers of random features.

From a theoretical perspective, there has been a lot of recent work analyzing the generalization performance of kernel approximation methods \citep{bach13,alaoui15,rudi15,optrff15,musco17,rudi17,bach17,avron17}. This work is generally concerned with proving upper bounds on the number of features needed to attain a certain level of generalization performance, or designing new sampling methods for the random features in order to attain stronger bounds. The theoretical work most relevant to ours is that of \citet{avron17}, which bounds the generalization performance of kernel approximation methods in the kernel ridge regression context, in terms of a notion of distance to the exact kernel matrix.  While the theory is informative, the method proposed in the paper for sampling random features scales exponentially with the dimension of the data, making it impractical in most regimes. We build on their theoretical framework to propose a novel method for achieving $4x$ to $30x$ compression at no loss in performance, relative to full-precision RFFs with circulant projections. Furthermore, while they propose a metric to measure the distance between a kernel matrix and an approximation to it, they only measure this metric on a small synthetic dataset. We modify their metric and measure it across four datasets for various kernel approximation methods. With our modified definition, we demonstrate that this metric correlates very strongly with generalization performance in practice.

Our work also provides an important new perspective to the question of which kernel method is best: the \Nystrom method or RFFs. An in-depth comparison of these methods was performed by \citet{nysvsrff12}, and argues that from both theoretical and empirical perspectives, the \Nystrom method is preferable to RFFs, for a fixed number of features. We show, through larger scale experiments (more features, larger datasets), that when memory utilization is considered, RFFs are better.

At the core of our contribution is the idea of using low-precision to use fewer bits, while still retaining the desired statistical properties of the learning algorithm.  There has been much recent interest in the topic of low-precision for accelerating training and/or inference, as well as for compressing models \citep{gupta15,hogwild15,hubara16,halp18,desa17,han15}.  From a hardware perspective, there has also been significant progress in developing chips which support low-precision operations, in order to use less energy, and perform computations faster; notable examples are Google's TPU \citep{tpu17} and Microsoft's Project Brainwave \citep{brainwave17}. Our work benefits enormously from these recent efforts, as these hardware accelerators could dramatically speed up the execution time of the training algorithm described in this work.