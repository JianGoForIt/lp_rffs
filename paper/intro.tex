%What is the problem?
%Why is it interesting and important?
%Why is it hard? (E.g., why do naive approaches fail?)
%Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
%What are the key components of my approach and results? Also include any specific limitations.

Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. Given a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, these methods generally construct a finite-dimensional feature map $z: \RR^d \rightarrow \RR^m$ such that $\dotp{z(x),z(x')} \approx k(x,x')$; linear models trained on these features then serve as an approximation to the exact kernel model. In order for the approximate model to generalize well, a very large number of features are often needed. When training in a resource-constrained environment, the memory required to store these features often becomes a bottleneck.

In this paper, we consider training large-scale kernel approximation models in memory-constrained environments, such as GPUs, using algorithms based on mini-batch updates. In particular, our goal is to get the strongest possible generalization performance for these methods, under a fixed memory budget. Importantly, this differs from the conventional approach taken in the literature, of studying how kernel approximation methods perform, as a function of the dimensionality $m$ of the feature map $z$.  Because some kernel approximation methods are dramatically more memory-intensive than others \textit{per feature}, choosing which approach to use when comparing these methods can have important implications.  For example, while prior work \citep{nysvsrff12} has shown that the \Nystrom method \citep{nystrom} is superior to random Fourier features (RFFs) \citep{rahimi07random} for a fixed number of features, we show that the opposite is true when memory utilization is considered. \avner{Concrete \#?}
%with more than $50\%$ relative generalization performance improvement.

%In this work, however, we show that the opposite is true when memory utilization is considered.
%We believe evaluating kernel approximation methods in this unconventional way can provide important new insights into the strengths and weaknesses of these methods.

%There are a number of important challenges involved in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training. First of all, answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models.

The core challenge in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training, is a theoretical one. Answering this question requires not only a deep understanding of the factors governing the generalization performance of kernel approximation models, but also an ability to leverage this understanding in order to develop new, better methods. While there has been much recent work studying generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, none of it has considered the memory constrained setting, or the question of whether varying the numerical precision could lead to improved performance per bit.  Furthermore, the ideas presented in these papers have yet to spawn new methods which empirically improve the performance of kernel approximation methods on real, large-scale tasks. Our goal in this work is to build on the existing theory to analyze the memory constrained setting, and develop new methods which attain better generalization performance per bit. 

The theoretical foundation of our proposed method, low-precision random Fourier features (LP-RFFs), is the recent work bounding the generalization performance of kernel approximation methods in the context of fixed design kernel ridge regression \citep{avron17,musco17}. Building on this work, we define a notion of spectral distance between a kernel matrix $K$ and an approximation $\tK$, and bound the generalization performance of $\tK$ using this distance. A core feature of this distance metric is that, given a regularization parameter $\lambda$, it upper and lower bounds the ratio between $\lambda_i(K)+\lambda$ and $\lambda_i(\tK) + \lambda$ for all eigenvalues of $K$ and $\tK$. An important consequence of this definition is that attaining tight bounds on generalization performance requires the kernel approximation matrix to be at least a certain rank, regardless of the approximation method used. This insight leads us to ask: under a fixed memory budget, how can we construct a high-rank approximation to the kernel matrix? The core feature of our proposed method, LP-RFFs, is to use $b$ bits per random feature, in place of 32 bits, thus allowing us to store $32/b$ more features in the same amount of memory. We additionally show that we can use a recently proposed low-precision training algorithm \citep{halp18} in order to reduce the memory footprint of the model parameters as well.

We study the theoretical and empirical performance of LP-RFFs relative to full-precision RFFs (FP-RFFs), and the \Nystrom method.  From a theoretical perspective, we show that there are important regimes under which quantization minimally degrades generalization performance; this allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features. In experiments across four benchmark datasets (TIMIT, YearPred, CovType, Census), we show that LP-RFFs consistently outperforms FP-RFFs, as well as the \Nystrom method, under a fixed memory budget. Specifically, we show, LP-RFFs can achieve the same performance as FP-RFFs and the \Nystrom method with up to 10.0x, and up to 436.9x less memory, respectively.

To summarize, our main contributions are as follows:
\begin{itemize}
	\item We propose using low-precision to generate high rank kernel approximations, under a fixed memory budget. We prove that there exist important regimes in which using low-precision gives minimal performance degradation.
	\item We empirically demonstrate on four datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs can be used in mini-batch SGD training to match the performance of FP-RFFs, while using 4x-30x less memory.
\end{itemize}

The rest of this paper is organized as follows:   In Section \ref{sec:prelim} we provide background on kernel approximation methods, along with the relevant generalization bounds. In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.

%As a result, in order for this bound to be tight, we need $\lambda_i(K) \approx \lambda_i(\tK)$ for all $i$ until $\lambda_i(\tK) << \lambda$.
%; given a regularization constant $\lambda$, we call this metrix $\lambda$-spectral distance, and denote it by $D_{\lambda}(K,\tK)$. 
%A core feature of this distance metric, is that it bounds the eigenvalues of $\tK$ in terms of $K$'s eigenvalues; specifically, given a regularization constant $\lambda$, it requires that $\lambda_i(\tK) + \lambda$ be no more that $(1+\Delta)$ times larger, or smaller, than $\lambda_i(K) + \lambda$.  In order for this to hold for a small value of $\Delta$, it follows that $\tK$ must be high rank; this is because otherwise, 

%$(1+D_{\lambda}(K,\tK))^{-1}(\lambda_i(K) + \lambda) \leq \lambda_i(\tK) + \lambda \leq (1+D_{\lambda}(K,\tK))(\lambda_i(K) + \lambda)$





%The first challenge in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training, is in understanding which \textit{existing} approximation methods perform best under memory constraints, and why. While there has been much recent work studying generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, none of it has considered the memory constrained setting. The second important challenge is in understanding whether these existing methods can be improved by using low-precision; given the memory constraint, it is natural to ask whether it would be beneficial use fewer bits to store each feature, so that more features could be computed under the same budget. The question of how precision affects generalization performance has not been considered.

%Our proposed method, low-precision random Fourier features (LP-RFFs), addresses both challenges discussed above.  We begin by comparing the two leading kernel approximation methods, the \Nystrom method and random Fourier features (RFFs), in this memory constrained setting.









%Answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models


%There are a number of important challenges involved in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training.


%First of all, answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models. 

%This would allow for understanding which existing methods are most memory efficient, and why.
%First, it requires an understanding of which of the currently existing methods 





%The core challenge in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training, is in deciding how to allocate bits across the components of the training pipeline. The primary components are:
%\begin{enumerate}
%	\item \textit{Feature generation}: Computing RFFs, for example, requires storing the random projection matrix $W \in \RR^{m \times d}$.
%	\item \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the minibatch, must be stored.
%	\item \textit{Learned parameters}: For binary classification and regression, the linear model learned on the $z(x)$ features is a vector $\theta \in \RR^m$; for $c$-class classification, it is a matrix $\theta \in \RR^{m\times c}$.
%\end{enumerate}
%Properly addressing this bit allocation problem requires analyzing what level of numerical precision is needed across these various components. While there has been much recent work studying generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, the question of how precision affects generalization performance has not been considered.

%. Second of all, there are various different components in the training pipeline which occupy memory: (1) \textit{Feature generation}: Computing RFFs, for example, requires storing the random projection matrix $W$; (2) \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the current minibatch, must be stored; (3) \textit{Learned parameters}: The linear model learned using the random features is a vector $\theta \in \RR^m$ for binary classification and regression problems, and is a matrix $\theta \in \RR^{m\times c}$ for $c$-class classification. %While there is extensive work reducing the time and space required for feature generation \citep{fastfood,yu15,sphereRKS}, we are not aware of any work aimed at reducing the memory occupied by the mini-batch of feature vectors, or by the model parameters, in the context of kernel approximation.



%Our approach addresses both challenges discussed above. From a theoretical perspective, we leverage the recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; an important consequence of these results is that attaining tight bounds on generalization performance requires the kernel approximation matrix to be at least a certain rank, regardless of the approximation method used. This insight leads us to ask: under a fixed memory budget, how can we construct a \textit{high-rank} approximation to the kernel matrix?  Our proposed solution, low-precision random Fourier features (LP-RFFS), comes in three parts: (1)  \textit{Feature generation}: we leverage existing work on using circulant random matrices to reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$ \citep{yu15}.\footnote{Technically, the space requirement is $33m$, because a vector of length $m$ of Radamacher random variables must also be stored; these variables only occupy 1 bit each. We write $32m$ for simplicity.} (2) \textit{Mini-batch storage}: we propose quantizing the full precision RFFs using $b$ bits per feature. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory. (3) \textit{Learned parameters}: For further savings, we can optionally store the model parameters themselves in low-precision, using the recently proposed LM-HALP for low-precision training \citep{halp18}. This is particularly important for multi-class classification problems, where the memory requirement for the model itself is large.

%We We  is to use low-precision. Specifically, we introduce \textit{low-precision random Fourier features} (LP-RFFS) as a way of reducing the space occupied by the \textit{mini-batches}. The core idea is to quantize the full precision RFFs using $b$ bits per features, to one of $2^b$ uniformly spaced values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory.  As an additional optimization, we reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$, by using circulant random matrices \citep{yu15}.

%Theoretically, our inspiration for this method came from recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; under the bounds in these papers, achieving a tight generalization bound requires a certain number of features, regardless of the approximation methods

%We study the theoretical and empirical performance of LP-RFFs relative to full-precision RFFs (FP-RFFs), and the \Nystrom method.  From a theoretical perspective, we show that there are important regimes under which quantization minimally degrades generalization performance; this allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features. In experiments across four benchmark datasets (TIMIT, YearPred, CovType, Census), we show that LP-RFFs consistently outperforms FP-RFFs, as well as the \Nystrom method, under a fixed memory budget. Specifically, we show, LP-RFFs can achieve the same performance as FP-RFFs and the \Nystrom method with up to 10.0x, and up to 436.9x less memory, respectively.
%
%To summarize, our main contributions are as follows:
%\begin{itemize}
%	\item We propose using low-precision to generate high rank kernel approximations, under a fixed memory budget. We prove that there exist important regimes in which using low-precision gives minimal performance degradation.
%	\item We empirically demonstrate on four datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs can be used in mini-batch SGD training to match the performance of FP-RFFs, while using 4x-30x less memory.
%\end{itemize}
%
%The rest of this paper is organized as follows:   In Section \ref{sec:prelim} we provide background on kernel approximation methods, along with the relevant generalization bounds. In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.