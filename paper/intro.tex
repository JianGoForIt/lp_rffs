Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. These methods generally construct finite-dimensional feature representations $z(x) \in \RR^m$ such that $\dotp{z(x),z(y)} \approx k(x,y)$.  Typically, the number of features $m$ is tunable, with larger $m$ resulting in a lower variance approximation of the kernel function.  

An important scalability issue for kernel approximation methods is that, in both theory and practice, it has been argued that a large number of features may be necessary for attaining strong generalization performance. As a result, the primary bottleneck in attaining strong generalization performance is \emph{computational}. In this paper, we propose comparing the generalization performance of kernel approximation methods under \emph{memory constraints}. The amount of memory used by a kernel approximation method is important for at least three reasons: (1) It determines how many features can be used when training on chips like GPUs with limited memory, (2) it is roughly proportional to the amount of time it takes to train a model, and (3) larger models are more expensive to deploy and use for inference. 


%An important scalability issue for kernel approximation methods is that, in both theory and practice, it has been argued that a large number of features may be necessary for attaining strong generalization performance. Thus, it is important, from both a systems and an algorithmic perspective, to have training algorithms that scale well with the number of features. One option for speeding up training is to leverage the parallelization capabilities of modern GPUs.  Unfortunately, these chips have limited memory, which upper bounds the number of features which can be used.  As a result, we argue that it is important to study the generalization performance of kernel approximation methods as a function of their \emph{memory utilization}. The amount of memory used by a kernel approximation method is important for at least two more reasons: (1) it is roughly proportional to the amount of time it takes to train a model, and (2) larger models are more expensive to deploy and use for inference.  

%We would like to highlight that our approach of comparing generalization performance vs. memory usage is very different from the much more common approach of comparing kernel approximation error vs. number of features.

In order to design kernel approximation methods which generalize well under a memory budget, it is first important to have a deep understanding of what properties of these approximation methods lead to strong generalization performance. While much of the literature focuses on kernel approximation error as the primary metric governing generalization, the recent work of \citeauthor{avron17}\citep{avron17} takes a very different approach. This work shows that given an approximation $\tK$ to a kernel matrix $K$ satisfying $(1-\Delta)(K+\lambda I) \preceq \tK + \lambda I \preceq (1+\Delta)(K+\lambda I)$, the generalization performance of a model trained using $\tK$ in place of $K$ can be bounded in terms of $\Delta$.  An important consequence of this definition of $\Delta$ is that attaining a small value of $\Delta$ necessarily requires a \emph{high rank} approximation.\footnote{In particular, in the case of kernel ridge regression, for $\tK$ to attain a value of $\Delta$, its rank $r$ must be large enough such that $\lambda_{r+1} \leq \frac{\lambda\Delta}{1-\Delta}$. This follows from $(1-\Delta)(\lambda_{r+1}+\lambda) \leq \tlambda_{r+1}+\lambda$, and the fact that $\tlambda_{r+1}=0$ if $\tK$ is rank $r$.  Here, $\lambda$ denotes the regularization constant, and $\lambda_{i}$ is the $i^{th}$ eigenvalues of $K$}. This insight leads us to ask: under a fixed memory budget, how can we construct higher rank approximation to $K$, relative to existing methods like \Nystrom \citep{nystrom} and random Fourier features (RFFs) \citep{rahimi07random}?

We propose using low-precision as a way of constructing high-rank approximations to $K$.  In particular, we introduce \emph{low-precision random Fourier features} (LP-RFFS), which simply correspond to a quantized version of the standard random Fourier features \citep{rahimi07random}.  For example, given a feature $z_i(x) = \sqrt{2/m}\cos(w_i^Tx+b_i)$, we quantize $z_i(x)$ using $b$ bits, to one of $2^b$ evenly distributed values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$. Thus, we can store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, allowing us to store $32/b$ more features in the same amount of memory.

We study the theoretical and empirical performance of LP-RFFs, relative to full-precision RFFs (FP-RFFs), as well as the \Nystrom method. From a theoretical perspective, we provide an upper bound on the value of $\Delta$ attained by LP-RFFs, showing there are important regimes in which these quantized features can be used to dramatically improve generalization performance under a memory budget. Empirically, across a number of tasks, we show that LP-RFFs consistently outperform FP-RFFs, which in term outperform the \Nystrom method, under fixed memory budgets. We explain these results through the theory of \citeauthor{avron17}; in particular, we measure the value of $\Delta$ attained by these methods, and show that the higher rank approximations systematically attain lower $\Delta$,\footnote{The primary exception is for very low-precision features, in which case the noise introduced by quantization affects the spectrum of $\tK$ and results in a larger $\Delta$.  See Theorem \avner{INSERT THEOREM \#}} and thus better generalization performance. This provides an important new perspective on the debate of whether FP-RFFs or \Nystrom is superior; while previous work argued that \Nystrom is superior because it attains better kernel approximation error and generalization performance for a fixed number of features \citep{nysvsrff12}, our work suggests that from a practical perspective, FP-RFFs are superior.  

To summarize, our main contributions are as follows:

% Jian suggestions
% performance under memory budget
% we observe performance correlated with $\Delta$
% We propose LP-RFFs to extend rank, and prove generalization bounds 
% We empirically validate that low-precision RFF can outperform low-precision RFF under memroy budget.

\begin{enumerate}
	\item We advocate for comparing kernel approximation methods in terms of their generalization performance under a memory budget.
	\item We propose LP-RFFs for generating high-rank approximations under memory constraints.  We prove generalization bounds for these features, and show they outperform FP-RFFs and the \Nystrom method on a variety of tasks, under a memory budget.
	\item We perform the first empirical comparison of generalization performance vs. $\Delta$, showing strong correlation across all the methods and datasets we tried.
\end{enumerate}

The rest of this paper is organized as follows: We review related work in Section \ref{sec:relwork}.  In Section \ref{sec:prelim} we provide background, including the generalization bounds of \citeauthor{avron17}\citep{avron17}.  In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}, and conclude in Section \ref{sec:conclusion}.

%Lastly, we note that as far as we are aware, our work is the first to systematically measure $\Delta$ across various kernel approximation methods, and show it's empirical correlation with generalization performance.  We believe this to be of independent interest.

% it correlates very strongly with both the rank and generalization performance of the kernel approximation methods.

%: (1) rank correlates strongly with $\Delta$, and (2) $\Delta$ correlates strongly with generalization performance.  As a result, when considering a fixed memory budget, LP-RFFs outperform FP-RFFs, which in turn outperform the \Nystrom method; this is simply because LP-RFFs have the highest rank, and \Nystrom features the lowest, under a fixed budget. This provides an important new perspective on the debate of whether FP-RFFs or \Nystrom is superior; while previous work argued that \Nystrom is superior because it attains better kernel approximation error and generalization performance for a fixed number of features \citep{nysvsrff12}, our work suggests that from a practical perspective, FP-RFFs are superior.  
%Lastly, we note that as far as we are aware, our work is the first to systematically measure $\Delta$ across various kernel approximation methods, and show it's empirical correlation with generalization performance.  We believe this to be of independent interest.


%We study the theoretical and empirical performance of LP-RFFs, relative to full-precision RFFs, as well as the \Nystrom method.  Theoretically, we upper bound the $\Delta$ value attained by these quantized features, showing there are important regimes in which LP-RFFs can be used to dramatically improve generalization performance under a memory budget.  Empirically, we compare its performance to both FP-RFFs and the \Nystrom method \citep{nystrom}, for a wide-range of numbers of features $m$ and precisions $b$, showing that LP-RFFs can achieve dramatically lower approximation error than existing methods, under a fixed budget.  
%As part of this empirical analysis, we perform the first systematic study measuring $\Delta$ across these kernel approximation methods on real datasets.  We show that there is a very strong correlation between $\Delta$ and the generalization performance of these methods, across a number of different tasks (classification/regression), providing strong empirical support for the above-mentioned theory.  In addition to supporting the theory, this study reveals some key differences between the \Nystrom method and random Fourier features.  First of all, we show that although for a fixed number of features, \Nystrom achieves \emph{significantly} lower kernel approximation error, the values of $\Delta$ attained by these methods are similar.  An important consequence of this, given that the \Nystrom method requires more memory, is that RFFs consistently outperform the \Nystrom method, when considering a fixed memory budget. This provides an important new perspective on the debate of which method is superior; while previous work argued that \Nystrom is superior because it attains better kernel approximation error and generalization performance for a fixed number of features \citep{nysvsrff12}, our work suggests that from a practical perspective, RFFs are superior.

%To summarize, our main contributions are as follows:
%\begin{enumerate}
%	\item We propose using low-precision as a way to generate more features under the same memory budget.  We prove generalization bounds for these low-precision features, and empirically demonstrate the superior performance of these features to the \Nystrom method and full precision RFFs under memory constraints.
%	\item We perform the first empirical comparison of generalization performance vs. $\Delta$, showing strong correlation across all the different methods and datasets we tried.
%\end{enumerate}


%From a practical perspective, we believe using low-precision  This will require implementing LP-RFFs on fast hardware (\eg, GPUs, TPUs, FPGAs) that leverages low-precision for faster and more energy efficient computation.  