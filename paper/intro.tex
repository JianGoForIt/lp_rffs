%What is the problem?
%--- large number of features lead to memory
%Why is it interesting and important?
%Why is it hard? (E.g., why do naive approaches fail?)
%Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
%What are the key components of my approach and results? Also include any specific limitations.

Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. Given a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, these methods typically construct a finite-dimensional feature map $z: \RR^d \rightarrow \RR^m$ such that $\dotp{z(x),z(x')} \approx k(x,x')$; linear models trained on these features then serve as an approximation to the exact kernel model. In order for the approximate model to generalize well, a very large number of features are often needed. Thus, the memory required to store these features often becomes a bottleneck.

In this paper, we consider training large-scale kernel approximation models in memory-constrained environments, such as GPUs, using algorithms based on mini-batch updates. Our goal is to get the strongest possible generalization performance for these methods, under a fixed memory budget. This approach of comparing kernel approximation methods based on memory utilization differs from the conventional approach of comparing methods based on the number of features $m$. Because some kernel approximation methods are dramatically more memory-intensive than others \textit{per feature}, choosing whether to compare methods based on memory utilization or feature dimensionality can have important implications. For example, while prior work \citep{nysvsrff12} has shown that the \Nystrom method \citep{nystrom} is superior to random Fourier features (RFFs) \citep{rahimi07random} for a fixed number of features, we show that the opposite is true under a fixed memory budget (\avner{Concrete \#?}). 

In the context of optimizing the performance \textit{per bit} of kernel approximation methods, it is natural to ask: Is it better to have many low-precision features, or few high-precision features? More formally, if we consider each feature to be an infinite-precision real number which we must quantize into $b$ bits, how should $b$ be chosen in order to optimize generalization performance? While there has been much recent work studying the generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, it has not considered the effect of feature quantization.

% We focus on quantizing RFFs, and not on \NystromNS, for two reasons: (1) RFFs are easier to quantize, because they are bounded in a smaller interval. (2) Computing RFF features is much less memory intensive than \Nystrom features, especially when the random projections are performed using structured matrices \citep{fastfood,yu15}.

The key component of our proposed method, \textit{low-precision random Fourier features} (LP-RFFs), is to use $b$ bits per random Fourier feature, instead of 32, thus allowing us to store $32/b$ more features in the same amount of memory. To analyze LP-RFFs, we leverage the recent work bounding the generalization performance of kernel approximation methods in the context of fixed design kernel ridge regression \citep{avron17,musco17}. Building on this work, we define a metric we call the $\lambda$-spectral distance between a kernel matrix $K$ and an approximation $\tK$, and bound the generalization performance of $\tK$ using this distance; the $\lambda$ here refers to the regularization parameter. Using this theoretical framework, we analyze the effect of using low-precision features on the $\lambda$-spectral distance of the resulting approximation matrix $\tK$. We show that when the noise introduced by quantization is much smaller than the regularization parameter $\lambda$, using low-precision will have negligible effects on the spectral distance, and thus on generalization.  This allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features.
%A core feature of this metric is it bounds the spectrum of $\tK+\lambda I$ above and below by the spectrum of $K+\lambda I$ multiplied by $1+\Delta$ and $(1+\Delta)^{-1}$.  Thus, in order for the $\lambda$-spectral distance to be small, the rank of $\tK$ must be high, regardless of the approximation method used. 

Empirically, we demonstrate across four benchmark datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs consistently outperform full precision RFFs (FP-RFFs), as well as the \Nystrom method, under a fixed memory budget. Specifically, we show that LP-RFFs can achieve the same performance as FP-RFFs and the \Nystrom method with 4x-30x and 50x-435x less memory, respectively. We explain these differences in performance in terms of the theory discussed above. We show that LP-RFFs consistently attain lower $\lambda$-spectral distance per bit than FP-RFFs and the \Nystrom method, and that across all our experiments there is a very strong correspondence between $\lambda$-spectral distance and generalization performance. This is in contrast to the spectral and Frobenius norms of $K-\tK$, which we observe to be much less predictive of generalization performance.  Lastly, we show that the model parameters themselves can be stored in low-precision by using a recently proposed training algorithm \citep{halp18}, with minimal effect on generalization. From a systems perspective, this is a very important detail, as it allows for all operations between the low-precision features and the model to be done using low-precision integer operations.

%We demonstrate empirically that although the above-mentioned theory is specific to fixed design kernel ridge regression, the $\lambda$-spectral distance remains predictive of performance for non-fixed design regression and logistic regression tasks.

To summarize, our main contributions are:
\begin{itemize}
	\item We propose using low-precision random Fourier features in order to get better generalization performance per bit, and prove a generalization bound which accounts for the noise introduced by feature quantization.
	\item We empirically demonstrate on four datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs can match the performance of FP-RFFs, while using 4x-30x less memory.
\end{itemize}

The rest of this paper is organized as follows: In Section \ref{sec:prelim} we review the relevant generalization bounds for kernel approximation methods, and provide a brief overview of random Fourier features and the \Nystrom method.  In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory. We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.



%Our goal in this work is to formally analyze the effect that quantization has on generalization performance, in order to then be able to optimize the number of bits used per feature and get improved performance under a memory budget.
% We focus on quantizing RFFs, and not on \NystromNS, for two reasons: (1) RFFs are easier to quantize, because they are bounded in a smaller interval. (2) Computing RFF features is much less memory intensive than \Nystrom features, especially when the random projections are performed using structured matrices \citep{fastfood,yu15}.

%Furthermore, the ideas presented in these papers have yet to spawn new methods which empirically improve the performance of kernel approximation methods on real, large-scale tasks. 

%with more than $50\%$ relative generalization performance improvement.


%In this work, however, we show that the opposite is true when memory utilization is considered.
%We believe evaluating kernel approximation methods in this unconventional way can provide important new insights into the strengths and weaknesses of these methods.

%There are a number of important challenges involved in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training. First of all, answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models.


%The core challenge in designing high-performance low-memory kernel approximation methods is a theoretical one. Answering this question requires not only a deep understanding of the factors governing the generalization performance of kernel approximation models, but also an ability to leverage this understanding in order to develop new, better methods. 
%the question of whether quantizing  improved performance per bit. 




%that it bounds the eigenvalues $\sigma_i$ and $\tsigma_i$ of $K$ and $\tK$ by $(1+D_{\lambda}(K,\tK))^{-1} \leq (\tsigma_i + \lambda)/(\sigma_i + \lambda) \leq 1+D_{\lambda}(K,\tK)$.

% between $\lambda_i(K)+\lambda$ and $\lambda_i(\tK) + \lambda$ for all eigenvalues of $K$ and $\tK$. A direct consequence of this is 




%An important consequence of this definition is that for the $\lambda$-spectral distance to be small, the kernel approximation matrix must be at least a certain rank, regardless of the approximation method used.  Using low-precision, we are able to attain higher rank approximations, under the same memory budget.  
% This insight leads us to ask: under a fixed memory budget, how can we construct a high-rank approximation to the kernel matrix? The core feature of our proposed method, LP-RFFs, is to use $b$ bits per random feature, in place of 32 bits, thus allowing us to store $32/b$ more features in the same amount of memory. We additionally show that we can use a recently proposed low-precision training algorithm \citep{halp18} in order to reduce the memory footprint of the model parameters as well.

%We study the theoretical and empirical performance of LP-RFFs relative to full-precision RFFs (FP-RFFs), and the \Nystrom method.  From a theoretical perspective, we show that there are important regimes under which quantization minimally degrades generalization performance; this allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features. 

%As a result, in order for this bound to be tight, we need $\lambda_i(K) \approx \lambda_i(\tK)$ for all $i$ until $\lambda_i(\tK) << \lambda$.
%; given a regularization constant $\lambda$, we call this metrix $\lambda$-spectral distance, and denote it by $D_{\lambda}(K,\tK)$. 
%A core feature of this distance metric, is that it bounds the eigenvalues of $\tK$ in terms of $K$'s eigenvalues; specifically, given a regularization constant $\lambda$, it requires that $\lambda_i(\tK) + \lambda$ be no more that $(1+\Delta)$ times larger, or smaller, than $\lambda_i(K) + \lambda$.  In order for this to hold for a small value of $\Delta$, it follows that $\tK$ must be high rank; this is because otherwise, 

%$(1+D_{\lambda}(K,\tK))^{-1}(\lambda_i(K) + \lambda) \leq \lambda_i(\tK) + \lambda \leq (1+D_{\lambda}(K,\tK))(\lambda_i(K) + \lambda)$





%The first challenge in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training, is in understanding which \textit{existing} approximation methods perform best under memory constraints, and why. While there has been much recent work studying generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, none of it has considered the memory constrained setting. The second important challenge is in understanding whether these existing methods can be improved by using low-precision; given the memory constraint, it is natural to ask whether it would be beneficial use fewer bits to store each feature, so that more features could be computed under the same budget. The question of how precision affects generalization performance has not been considered.

%Our proposed method, low-precision random Fourier features (LP-RFFs), addresses both challenges discussed above.  We begin by comparing the two leading kernel approximation methods, the \Nystrom method and random Fourier features (RFFs), in this memory constrained setting.









%Answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models


%There are a number of important challenges involved in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training.


%First of all, answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models. 

%This would allow for understanding which existing methods are most memory efficient, and why.
%First, it requires an understanding of which of the currently existing methods 





%The core challenge in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training, is in deciding how to allocate bits across the components of the training pipeline. The primary components are:
%\begin{enumerate}
%	\item \textit{Feature generation}: Computing RFFs, for example, requires storing the random projection matrix $W \in \RR^{m \times d}$.
%	\item \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the minibatch, must be stored.
%	\item \textit{Learned parameters}: For binary classification and regression, the linear model learned on the $z(x)$ features is a vector $\theta \in \RR^m$; for $c$-class classification, it is a matrix $\theta \in \RR^{m\times c}$.
%\end{enumerate}
%Properly addressing this bit allocation problem requires analyzing what level of numerical precision is needed across these various components. While there has been much recent work studying generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, the question of how precision affects generalization performance has not been considered.

%. Second of all, there are various different components in the training pipeline which occupy memory: (1) \textit{Feature generation}: Computing RFFs, for example, requires storing the random projection matrix $W$; (2) \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the current minibatch, must be stored; (3) \textit{Learned parameters}: The linear model learned using the random features is a vector $\theta \in \RR^m$ for binary classification and regression problems, and is a matrix $\theta \in \RR^{m\times c}$ for $c$-class classification. %While there is extensive work reducing the time and space required for feature generation \citep{fastfood,yu15,sphereRKS}, we are not aware of any work aimed at reducing the memory occupied by the mini-batch of feature vectors, or by the model parameters, in the context of kernel approximation.



%Our approach addresses both challenges discussed above. From a theoretical perspective, we leverage the recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; an important consequence of these results is that attaining tight bounds on generalization performance requires the kernel approximation matrix to be at least a certain rank, regardless of the approximation method used. This insight leads us to ask: under a fixed memory budget, how can we construct a \textit{high-rank} approximation to the kernel matrix?  Our proposed solution, low-precision random Fourier features (LP-RFFS), comes in three parts: (1)  \textit{Feature generation}: we leverage existing work on using circulant random matrices to reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$ \citep{yu15}.\footnote{Technically, the space requirement is $33m$, because a vector of length $m$ of Radamacher random variables must also be stored; these variables only occupy 1 bit each. We write $32m$ for simplicity.} (2) \textit{Mini-batch storage}: we propose quantizing the full precision RFFs using $b$ bits per feature. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory. (3) \textit{Learned parameters}: For further savings, we can optionally store the model parameters themselves in low-precision, using the recently proposed LM-HALP for low-precision training \citep{halp18}. This is particularly important for multi-class classification problems, where the memory requirement for the model itself is large.

%We We  is to use low-precision. Specifically, we introduce \textit{low-precision random Fourier features} (LP-RFFS) as a way of reducing the space occupied by the \textit{mini-batches}. The core idea is to quantize the full precision RFFs using $b$ bits per features, to one of $2^b$ uniformly spaced values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory.  As an additional optimization, we reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$, by using circulant random matrices \citep{yu15}.

%Theoretically, our inspiration for this method came from recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; under the bounds in these papers, achieving a tight generalization bound requires a certain number of features, regardless of the approximation methods

%We study the theoretical and empirical performance of LP-RFFs relative to full-precision RFFs (FP-RFFs), and the \Nystrom method.  From a theoretical perspective, we show that there are important regimes under which quantization minimally degrades generalization performance; this allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features. In experiments across four benchmark datasets (TIMIT, YearPred, CovType, Census), we show that LP-RFFs consistently outperforms FP-RFFs, as well as the \Nystrom method, under a fixed memory budget. Specifically, we show, LP-RFFs can achieve the same performance as FP-RFFs and the \Nystrom method with up to 10.0x, and up to 436.9x less memory, respectively.
%
%To summarize, our main contributions are as follows:
%\begin{itemize}
%	\item We propose using low-precision to generate high rank kernel approximations, under a fixed memory budget. We prove that there exist important regimes in which using low-precision gives minimal performance degradation.
%	\item We empirically demonstrate on four datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs can be used in mini-batch SGD training to match the performance of FP-RFFs, while using 4x-30x less memory.
%\end{itemize}
%
%The rest of this paper is organized as follows:   In Section \ref{sec:prelim} we provide background on kernel approximation methods, along with the relevant generalization bounds. In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.