\begin{itemize}
	\item Kernel methods are powerful, but don't scale well.  Need kernel approximation methods.
	\item For kernel approximation methods to do well, generally need lots of features.  When performing GPU training (limited memory), or when considering model deployment, the memory requirement for these models becomes a large bottleneck.  An example of this is the recent work in large-scale kernel methods for speech recognition, which are constrained in terms of the number of features they can fit on GPU.
	\item As a result, we perform a thorough empirical comparison of Nystrom and RFF, two leading kernel approximation methods, in the memory constrained setting.  Surprisingly, we find that although the Nystrom method often has lower kernel approximation error (Frobenius/Spectral norm), it performs worse than RFF in this setting.  We note that this runs counter to the common belief that Nystrom is better than RFF.
	\item We explain these observations using the recent theoretical results of Avron et al.  We note that the bounds in this paper differ in important ways from prior bounds which are generally based on the kernel approximation error (and thus useless to explain the observed phenomenon).
	\item We note that the primary insight provided by these recent bounds is that it is crucial to have a high-rank decomposition whose spectrum roughly aligns with the exact spectrum, even if it has high kernel approximation error.  Note that this runs contrary to a lot of the literature kernel approximation literature, which is often narrowly concerned with reducing the kernel approximation error.  Furthermore, we consider the computational cost, in terms of memory, of computing these features, another aspect which is generally not considered when comparing Nystrom and RFF.
	\item We leverage this insight to propose LP-RFF.
	\item We analyze LP-RFF theoretically, showing that you can upper bound the value $\Delta$ for LP-RFF in terms of the number of bits per feature used.
	\item We show that LP-RFF are able to perform on par with full-precision features, at a fraction of the memory.
	\item This opens the door for training kernel approximation models with many more features than are currently being used.  To fully leverage these insights to attain faster, lower-memory, and lower-energy training, it will be necessary to implement efficient version of our algorithm (mention TPUs, GPUs, FPGAs, etc). \avner{Are TPUs already being used for neural net training in low-precision?}
	\item Include sexy plot in intro summarizing the main results of the paper?  Generalization performance vs. memory usage on TIMIT, for \Nystrom, RFF, and LP-RFF?
\end{itemize}

Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. These methods generally construct finite-dimensional feature representations $z(x) \in \RR^m$ such that $\dotp{z(x),z(y)} \approx k(x,y)$.  Typically, the number of features $m$ is tunable, with larger $m$ resulting in a lower variance approximation of the kernel function.  In both theory and practice, it has been argued that a large number of features are often necessary for attaining strong generalization performance.  In order to perform fast training with a large number of features, one can leverage the parallelization capabilities of modern GPUs.  Unfortunately, these chips have limited memory, which upper bounds the number of features which can be used.  As a result, we argue that it is important to study the generalization performance of kernel approximation methods as a function of their memory utilization. The amount of memory used by a kernel approximation method is important for at least two more reasons: (1) it is roughly proportional to the amount of time it takes to train a model, and (2) larger models are more expensive to deploy and use for inference.  We would like to highlight that our approach of comparing generalization performance vs. memory usage is very different from the much more common approach of comparing kernel approximation error vs. number of features.

Our main contributions are as follows:
\begin{enumerate}
	\item We perform the first empirical comparison of generalization performance vs. $\Delta$, showing a very strong correlation across all the different kernel approximation methods we tried (\Nystrom, FP-RFF, LP-RFF).
	\item We for the first time study the performance of kernel approximation methods as a function of their memory usage, revealing that random Fourier features are actually superior to the \Nystrom method under this setting.
	\item We propose using low-precision as a way to generate more features under the same memory budget.  We prove generalization bounds for these low-precision features, and empirically demonstrate the superior performance of these features to existing method (under memory constraint).
\end{enumerate}
	