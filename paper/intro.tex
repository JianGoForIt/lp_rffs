%What is the problem?
%Why is it interesting and important?
%Why is it hard? (E.g., why do naive approaches fail?)
%Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
%What are the key components of my approach and results? Also include any specific limitations.

Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. Given a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, these methods generally construct a finite-dimensional feature map $z: \RR^d \rightarrow \RR^m$ such that $\dotp{z(x),z(x')} \approx k(x,x')$; linear models trained on these features then serve as an approximation to the exact kernel model. In order for the approximate model to generalize as well as the exact model, a very large number of features are often needed, posing scalability challenges for these approximation methods. One common way to speed up training in this setting is using GPUs, but these chips have limited memory, effectively limiting the number of features which can be used. \avner{Cite speech recognition?}

In this paper, we consider training large-scale kernel approximation models in memory-constrained environments, such as GPUs, using algorithms based on mini-batch updates. In particular, our goal is to get the strongest possible generalization performance for these methods, under a fixed memory budget.  Importantly, this differs from the conventional approach taken in the literature, of studying how kernel approximation methods perform, as a function of the dimensionality $m$ of the feature map $z$. While the typical approach reveals important statistical properties of the kernel approximation methods being analyzed, it ignores the fact that different methods can have drastically different memory footprints with the same dimensionality for the feature map. Choosing which approach to use when comparing methods can have important implications: for example, \citet{nysvsrff12} compare the \Nystrom method \citep{nystrom} with random Fourier features (RFFs) \citep{rahimi07random}, two important kernel approximation methods, and show that the \Nystrom method \citep{nystrom} outperforms random Fourier features (RFFs) \citep{rahimi07random} for a fixed number of features.  However, we observe that the opposite is true under a memory budget, with RFF systematically outperforming the \Nystrom method. \avner{Concrete \#?}
%with more than $50\%$ relative generalization performance improvement.

%In this work, however, we show that the opposite is true when memory utilization is considered.
%We believe evaluating kernel approximation methods in this unconventional way can provide important new insights into the strengths and weaknesses of these methods.

There are a number of important challenges involved in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training. First of all, answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models; this is a very active area of research, with many remaining open problems \citep{rudi17,avron17,musco17,bach17}. Second of all, there are various different components in the training pipeline which occupy memory: (1) \textit{Feature generation}: Computing RFFs, for example, requires storing the random projection matrix $W$; (2) \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the current minibatch, must be stored; (3) \textit{Learned parameters}: The linear model learned using the random features is a vector $\theta \in \RR^m$ for binary classification and regression problems, and is a matrix $\theta \in \RR^{m\times c}$ for $c$-class classification. %While there is extensive work reducing the time and space required for feature generation \citep{fastfood,yu15,sphereRKS}, we are not aware of any work aimed at reducing the memory occupied by the mini-batch of feature vectors, or by the model parameters, in the context of kernel approximation.

Our approach addresses both challenges discussed above. From a theoretical perspective, we leverage the recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; an important consequence of these results is that attaining tight bounds on generalization performance requires the kernel approximation matrix to be at least a certain rank, regardless of the approximation method used. This insight leads us to ask: under a fixed memory budget, how can we construct a \textit{high-rank} approximation to the kernel matrix?  Our proposed solution, low-precision random Fourier features (LP-RFFS), comes in three parts: (1)  \textit{Feature generation}: we leverage existing work on using circulant random matrices to reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$ \citep{yu15}.\footnote{Technically, the space requirement is $33m$, because a vector of length $m$ of Radamacher random variables must also be stored; these variables only occupy 1 bit each. We write $32m$ for simplicity.} (2) \textit{Mini-batch storage}: we propose quantizing the full precision RFFs using $b$ bits per feature. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory. (3) \textit{Learned parameters}: For further savings, we can optionally store the model parameters themselves in low-precision, using the recently proposed LM-HALP for low-precision training \citep{halp18}. This is particularly important for multi-class classification problems, where the memory requirement for the model itself is large.

%We We  is to use low-precision. Specifically, we introduce \textit{low-precision random Fourier features} (LP-RFFS) as a way of reducing the space occupied by the \textit{mini-batches}. The core idea is to quantize the full precision RFFs using $b$ bits per features, to one of $2^b$ uniformly spaced values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory.  As an additional optimization, we reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$, by using circulant random matrices \citep{yu15}.

%Theoretically, our inspiration for this method came from recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; under the bounds in these papers, achieving a tight generalization bound requires a certain number of features, regardless of the approximation methods

We study the theoretical and empirical performance of LP-RFFs relative to full-precision RFFs (FP-RFFs), and the \Nystrom method.  From a theoretical perspective, we show that there are important regimes under which quantization minimally degrades generalization performance; this allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features. In experiments across four benchmark datasets (TIMIT, YearPred, CovType, Census), we show that LP-RFFs consistently outperforms FP-RFFs, as well as the \Nystrom method, under a fixed memory budget. Specifically, we show, LP-RFFs can achieve the same performance as FP-RFFs and the \Nystrom method with up to 10.0x, and up to 436.9x less memory, respectively.

To summarize, our main contributions are as follows:
\begin{itemize}
	\item We propose using low-precision to generate high rank kernel approximations, under a fixed memory budget. We prove that there exist important regimes in which using low-precision gives minimal performance degradation.
	\item We empirically demonstrate on four datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs can be used in mini-batch SGD training to match the performance of FP-RFFs, while using 4x-30x less memory.
\end{itemize}

The rest of this paper is organized as follows:   In Section \ref{sec:prelim} we provide background on kernel approximation methods, along with the relevant generalization bounds. In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.