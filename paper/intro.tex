%What is the problem?
%Why is it interesting and important?
%Why is it hard? (E.g., why do naive approaches fail?)
%Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
%What are the key components of my approach and results? Also include any specific limitations.

Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. Given a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, these methods typically construct a finite-dimensional feature map $z: \RR^d \rightarrow \RR^m$ such that $\dotp{z(x),z(x')} \approx k(x,x')$; linear models trained on these features then serve as an approximation to the exact kernel model. In order for the approximate model to generalize well, a very large number of features are often needed \citep{rahimi08kitchen,block16,may2017}. Thus, the memory required to store these features often becomes a bottleneck.

In this paper, we consider training large-scale kernel approximation models in memory-constrained environments using standard mini-batch based algorithms. Our goal is to get the best possible generalization performance for these methods, under a fixed memory budget. This approach of comparing kernel approximation methods based on memory utilization differs from the conventional approach of comparing methods based on the number of features $m$. Because some kernel approximation methods are dramatically more memory-intensive than others \textit{per feature}, choosing whether to compare methods based on memory utilization or feature dimensionality can have important implications. For example, while prior work \citep{nysvsrff12} has shown that the \Nystrom method \citep{nystrom} is superior to random Fourier features (RFFs) \citep{rahimi07random} for a fixed number of features, we show that the opposite is true under a fixed memory budget. Strikingly, we observe that $\num[group-separator={,}]{50000}$ standard RFFs can achieve the same performance as $\num[group-separator={,}]{20000}$ \Nystrom features with 10x less memory on the TIMIT classification task.

In the context of optimizing the performance \textit{per bit} of kernel approximation methods, it is natural to ask: Is it better to have many low-precision features, or few high-precision features? More formally, if we consider each feature to be an infinite-precision real number which we must quantize into $b$ bits, how should $b$ be chosen in order to optimize generalization performance? While there has been much recent work studying the generalization performance for kernel approximation methods \citep{rudi17,avron17,musco17,bach17}, it has not considered the effect of feature quantization.

The key component of our proposed method, \textit{low-precision random Fourier features} (LP-RFFs), is to use $b$ bits per random Fourier feature, instead of 32, thus allowing us to store $32/b$ more features in the same amount of memory. In contrast to \Nystrom features, RFFs are particularly amenable to quantization because their magnitudes are bounded by $O(1/\sqrt{m})$.
%\footnote{In experiments with the \Nystrom method we observe a large dynamic range of values between -1 and 1 for each \Nystrom feature, making it challenging to design good quantization schemes and prove concentration bounds.}
To analyze LP-RFFs, we define the \textit{relative spectral distance} (Definition~\ref{def:specdist}) between a kernel matrix $K$ and an approximation $\tK$, a metric which bounds the spectrum of $\tK$ relative to that of $K$. We observe empirically that this distance is much more predictive of generalization performance than the Frobenius or spectral norms of $K-\tK$ (Figure~\ref{fig:specdist}). We then build on recent work \citep{avron17,musco17} to bound the generalization performance of $\tK$ using this distance. Using this theoretical framework, we bound the relative spectral distance between the LP-RFF approximation matrix and the true kernel matrix; we show that when the noise introduced by quantization is much smaller than the regularization parameter $\lambda$, using low-precision will have negligible effects on the relative spectral distance, and thus on generalization. This allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features.

Empirically, we demonstrate across four benchmark datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs consistently outperform full-precision RFFs (FP-RFFs), as well as the \Nystrom method, under a fixed memory budget. Specifically, we show that LP-RFFs can achieve the same performance as FP-RFFs and the \Nystrom method with 5x-10x and 50x-460x less memory, respectively. We explain these differences in performance in terms of the theory discussed above. We show that LP-RFFs consistently attain lower relative spectral distance per bit than FP-RFFs and the \Nystrom method. Lastly, we show that the model parameters themselves can be stored in low-precision by using a recently proposed training algorithm \citep{halp18}, with minimal effect on generalization. From a systems perspective, this is a very important detail, as it allows for the operations between the low-precision features and the model to be done using low-precision integer operations.

In principle, it should be possible to extend similar ideas to the \Nystrom method. However, our initial experiments suggest that the dynamic range of the \Nystrom features is large and so a more sophisticated scheme would be needed. One way to reduce the dynamic range by a factor of $\sqrt{r}$ is to use the ensemble \Nystrom method \citep{ensemble09} with $r$ blocks; this has the additional benefit of reducing the memory needed to compute the features. Unfortunately, the ensemble method yields approximation matrices with large relative spectral distance to the true kernel matrix. These initial explorations lead us to believe that more thought is needed in order to effectively use low-precision for the \Nystrom method.

%To summarize, our main contributions are:
%\begin{itemize}
%	\item We propose using low-precision random Fourier features in order to get better generalization performance per bit, and prove a generalization bound which accounts for the noise introduced by feature quantization.
%	\item We empirically demonstrate on four datasets (TIMIT, YearPred, CovType, Census) that LP-RFFs can match the best performance of FP-RFFs and the \Nystrom method, while using 5x-10x and 50x-460x less memory, respectively.
%\end{itemize}

The rest of this paper is organized as follows: In Section \ref{sec:prelim} we discuss the memory utilization for kernel approximation methods, and review the relevant generalization bounds.  In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory. We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.
