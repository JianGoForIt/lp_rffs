Kernel methods are a powerful family of machine learning methods.  Unfortunately, they do not scale well to large datasets, typically requiring time and memory that are at least quadratic in the number of training points. To address these scalability issues, kernel approximations methods have been proposed. Given a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, these methods generally construct finite-dimensional feature $z: \RR^d \rightarrow \RR^m$ such that $\dotp{z(x),z(x')} \approx k(x,x')$; linear models trained on these features then serve as an approximation to the exact kernel model. 

In this paper, we consider training large-scale kernel approximation models in memory-constrained environments, such as GPUs, using algorithms based on mini-batch updates. In particular, our goal is to get the strongest possible generalization performance for these methods, under a fixed memory budget.  Importantly, this differs from the conventional approach taken in the literature, of studying how kernel approximation methods perform, as a function of the dimensionality $m$ of the feature map $z$. While this approach reveals important statistical properties of the kernel approximation method being analyzed, it ignores the fact that different methods can have drastically different memory footprints. This can have important implications: for example, \citet{nysvsrff12} compare the \Nystrom method \citep{nystrom} with random Fourier features (RFFs) \citep{rahimi07random}, two important kernel approximation methods, and show that the \Nystrom method \citep{nystrom} outperforms random Fourier features (RFFs) \citep{rahimi07random} for a fixed number of features.  However, we observe that the opposite is true under a memory budget, with RFF systematically outperforming the \Nystrom method.


%In this work, however, we show that the opposite is true when memory utilization is considered.
%We believe evaluating kernel approximation methods in this unconventional way can provide important new insights into the strengths and weaknesses of these methods.

There are a number of important challenges involved in designing kernel approximation methods which attain strong generalization performance, while maintaining a small memory footprint during training. First of all, answering this question requires a strong understanding of the factors governing the generalization performance of kernel approximation models; this is a very active area of research, with many remaining open problems \citep{rudi17,avron17,musco17,bach17}. Second of all, there are various different components to the training algorithm which occupy memory: (1) \textit{Feature generation}: Computing RFFs, for example, requires the random projection matrix $W$; (2) \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the current minibatch, must be stored; (3) \textit{Learned parameters}: The learned parameter matrix can be quite large in the case of multi-class classification. While there is extensive work reducing the time and space required for feature generation \citep{fastfood,yu15,sphereRKS}, we are not aware of any work aimed at reducing the memory occupied by the mini-batch of feature vectors, or by the model parameters, in the context of kernel approximation.

Our approach addresses both challenges discussed above. From a theoretical perspective, we leverage the recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; an important consequence of these results is that attaining tight bounds on generalization performance requires the kernel approximation matrix to be at least a certain rank, regardless of the approximation method used. This insight leads us to ask: under a fixed memory budget, how can we construct \textit{high-rank} approximation to the kernel matrix?  Our proposed solution, low-precision random Fourier features (LP-RFFS), comes in three parts: (1)  \textit{Feature generation}: we leverage existing work on using circulant random matrices to reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$ \citep{yu15}. (2) \textit{Mini-batch storage}: we propose quantizing the full precision RFFs using $b$ bits per features. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory. (3) \textit{Learned parameters}: For further savings, we can optionally store the model parameters themselves in low-precision, and use the recently proposed LM-HALP for low-precision training \citep{halp18}.

%We We  is to use low-precision. Specifically, we introduce \textit{low-precision random Fourier features} (LP-RFFS) as a way of reducing the space occupied by the \textit{mini-batches}. The core idea is to quantize the full precision RFFs using $b$ bits per features, to one of $2^b$ uniformly spaced values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$. This allows us to store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, thus letting us store $32/b$ more features in the same amount of memory.  As an additional optimization, we reduce the overhead of storing the projection matrix $W$ from $32md$ to $32m$, by using circulant random matrices \citep{yu15}.

%Theoretically, our inspiration for this method came from recent generalization bounds for kernel approximation methods in the context of kernel ridge regression \citep{avron17,musco17}; under the bounds in these papers, achieving a tight generalization bound requires a certain number of features, regardless of the approximation methods

We study the theoretical and empirical performance of LP-RFFs relative to full-precision RFFs (FP-RFFs), and the \Nystrom method.  From a theoretical perspective, we show that there are important regimes under which quantization minimally degrades generalization performance; this allows for attaining stronger performance under a fixed memory budget using LP-RFFs, by using a larger number of features. In experiments across four benchmark datasets (TIMIT,YearPred,CovType,Census), we show that LP-RFFs consistently outperforms FP-RFFs, as well as the \Nystrom method, under a fixed memory budget. Specifically, we show, LP-RFFs can achieve the same performance as FP-RFFs with $4x$ to $30x$ less memory.

To summarize, our main contributions are as follows:
\begin{itemize}
	%\item We conduct the first systematic empirical study of the generalization performance of kernel approximation methods under memory constraints for mini-batch-based training.
	\item We propose using low-precision to generate higher rank kernel approximations, under a fixed memory budget. We prove that there exist important regimes in which using low-precision gives minimal performance degradation.
	\item We empirically demonstrate on three large datasets (TIMIT, YearPred, CovType) that LP-RFFs can be used in mini-batch SGD training to match the performance of FP-RFFs, while using 4x-30x less memory.
\end{itemize}

The rest of this paper is organized as follows:   In Section \ref{sec:prelim} we provide background on kernel approximation methods, along with the relevant generalization bounds. In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}.  We review related work in Section \ref{sec:relwork}, and conclude in Section \ref{sec:conclusion}.


%\begin{itemize}
%	\item Kernel methods are powerful but don't scale well; thus, approximation methods are needed. In practice, people use SGD-based training algorithms in resource-constrained environments to train large-scale kernel approximation methods (eg, GPUs, edge devices, data-centers where energy is expensive).
%	\item In this paper, we investigate how to get the best possible generalization performance for kernel approximation methods, using SGD training under memory constraints.  We highlight that this differs from previous work, which typically considers generalization performance/kernel approximation error as a function of the number of features used.  Importantly, only considering the number of features ignores important differences in the memory-utilization of different kernel approximation methods, for example \Nystrom and RFF.  We note that once you consider memory, RFF outperforms \Nystrom, going against conventional opinion that \Nystrom outperforms RFF \citep{nysvsrff12}.
%	\item Why is this problem important? Solving this problem would allow for training better models under any resource constraint.
%	\item Why is it hard: Requires minimizing memory usage of three components: (1) feature generation parameters, (2) mini-batch memory, (3) model.  Furthermore, it requires understanding what properties of kernel approximation methods lead to strong performance.
%	\item Why hasn't been solved: Existing methods for reducing memory usage typically only focus on the feature generation process (\eg, fastfood).  This doesn't address the memory utilization of the mini-batch features, or of the model.  Also, theory is still pretty new, so it's lessons haven't yet made their way into practice.
%	\item Our approach: we quantize random Fourier features in order to reduce the memory utilization of the mini-batch (and potentially, of the model). We motivate this solution by citing recent theory, which lower-bounds the rank necessary for achieving a tight generalization bound.  Low-precision thus serves as a way of generating a higher rank approximation, under a fixed budget. 
%	\item We perform theoretical analysis on low precision RFF, showing that under certain regimes, with high probability, low precision RFF from random quantization shows small generalization degradation; this allows for attaining stronger performance under a fixed memory budget.
%	%To support our focus on upper-bounding $\Delta$ for LP-RFF, we show that $\Delta$ correlates very strong with generalization performance across all the different approximation methods we consider.
%	\item We empirically demonstrate on 3 big dataset to show that, under memory budget, LP RFF shows better generalization performance than FP RFF and \Nystrom. Specifically, we show, LP RFF can achieve the same performance with 100x less memory footprint (or we show, under the memory budget, LP RFF has 2x better generalization performance than FP RFF and \Nystrom).
%\end{itemize}
%
%Our main contributions are as follows:
%\begin{itemize}
%	\item We conduct the first systematic empirical study of kernel approximation performance under memory budget for SGD-based large scale kernel machine training. 
%	\item We propose using low-precision to generate higher rank kernel approximations, under a fixed memory budget.  We prove that there exist important regimes in which using low-precision gives minimal performance degradation, thus implying we can achieve better performance under a memory budget.
%	\item We empirically demonstrate on 3 big dataset to show that, under memory budget, LP RFF show up to 2x better generalization performance than FP RFF and Nystrom.
%\end{itemize}


%To highlight the importance of this change, consider the question of comparing the \Nystrom method \citep{nystrom} with random Fourier features \citep{rahimi07random}, two notable kernel approximation methods.  When comparing these methods as a function of the number of features, \citet{nysvsrff12} showed that the \Nystrom method is superior, both in theory and empirically.  


%We highlight that this differs from previous work, which typically considers generalization performance/kernel approximation error as a function of the number of features used.  Importantly, only considering the number of features ignores important differences in the memory-utilization of different kernel approximation methods, for example \Nystrom and RFF.  We note that once you consider memory, RFF outperforms \Nystrom, going against conventional opinion that \Nystrom outperforms RFF \citep{nysvsrff12}. 



%Over the years, various kernel approximation methods have been proposed and studied, most notably the \Nystrom method \citep{nystrom} and random Fourier features (RFFs) \citep{rahimi07random}.  There has been extensive empirical and theoretical work studying the kernel approximation as well as generalization performance of these methods\citep{rahimi08kitchen,nysvsrff12,avron17,rudi17,huang14kernel,may2017}. On both fronts, the central question is how performance varies as a function of the dimensionality $m$ of the feature map $z$. For example, \citet{nysvsrff12} argue that \Nystrom performs better than RFFs, both in theory and in practice, for a fixed number of features. These comparisons, however, ignore an important fact; some kernel approximation methods are much more expensive than others.  When training kernel approximation models in resource-constrained settings, these

%n this work, however, we show that RFF actually outperforms \Nystrom under fixed memory budgets.





%Typically, the number of features $m$ is tunable, with larger $m$ resulting in a lower variance approximation of the kernel function.  

%An important scalability issue for kernel approximation methods is that, in both theory and practice, it has been argued that a large number of features may be necessary for attaining strong generalization performance. As a result, the primary bottleneck in attaining strong generalization performance is \emph{computational}. In this paper, we propose comparing the generalization performance of kernel approximation methods under \emph{memory constraints}. The amount of memory used by a kernel approximation method is important for at least three reasons: (1) It determines how many features can be used when training on chips like GPUs with limited memory, (2) it is roughly proportional to the amount of time it takes to train a model, and (3) larger models are more expensive to deploy and use for inference. 


%We would like to highlight that our approach of comparing generalization performance vs. memory usage is very different from the much more common approach of comparing kernel approximation error vs. number of features.


%An important scalability issue for kernel approximation methods is that, in both theory and practice, it has been argued that a large number of features may be necessary for attaining strong generalization performance. Thus, it is important, from both a systems and an algorithmic perspective, to have training algorithms that scale well with the number of features. One option for speeding up training is to leverage the parallelization capabilities of modern GPUs.  Unfortunately, these chips have limited memory, which upper bounds the number of features which can be used.  As a result, we argue that it is important to study the generalization performance of kernel approximation methods as a function of their \emph{memory utilization}. The amount of memory used by a kernel approximation method is important for at least two more reasons: (1) it is roughly proportional to the amount of time it takes to train a model, and (2) larger models are more expensive to deploy and use for inference.  

%

%In order to design kernel approximation methods which generalize well under a memory budget, it is first important to have a deep understanding of what properties of these approximation methods lead to strong generalization performance. While much of the literature focuses on kernel approximation error as the primary metric governing generalization, the recent work of \citeauthor{avron17}\citep{avron17} takes a very different approach. This work shows that given an approximation $\tK$ to a kernel matrix $K$ satisfying $(1-\Delta)(K+\lambda I) \preceq \tK + \lambda I \preceq (1+\Delta)(K+\lambda I)$, the generalization performance of a model trained using $\tK$ in place of $K$ can be bounded in terms of $\Delta$.  An important consequence of this definition of $\Delta$ is that attaining a small value of $\Delta$ necessarily requires a \emph{high rank} approximation.\footnote{In particular, in the case of kernel ridge regression, for $\tK$ to attain a value of $\Delta$, its rank $r$ must be large enough such that $\lambda_{r+1} \leq \frac{\lambda\Delta}{1-\Delta}$. This follows from $(1-\Delta)(\lambda_{r+1}+\lambda) \leq \tlambda_{r+1}+\lambda$, and the fact that $\tlambda_{r+1}=0$ if $\tK$ is rank $r$.  Here, $\lambda$ denotes the regularization constant, and $\lambda_{i}$ is the $i^{th}$ eigenvalues of $K$}. This insight leads us to ask: under a fixed memory budget, how can we construct higher rank approximation to $K$, relative to existing methods like \Nystrom \citep{nystrom} and random Fourier features (RFFs) \citep{rahimi07random}?

%We propose using low-precision as a way of constructing high-rank approximations to $K$.  In particular, we introduce \emph{low-precision random Fourier features} (LP-RFFS), which simply correspond to a quantized version of the standard random Fourier features \citep{rahimi07random}.  For example, given a feature $z_i(x) = \sqrt{2/m}\cos(w_i^Tx+b_i)$, we quantize $z_i(x)$ using $b$ bits, to one of $2^b$ evenly distributed values in the interval $[-\sqrt{2/m},\sqrt{2/m}]$. Thus, we can store a feature vector $z(x)$ in $bm$ bits instead of $32m$ bits, allowing us to store $32/b$ more features in the same amount of memory.
%
%We study the theoretical and empirical performance of LP-RFFs, relative to full-precision RFFs (FP-RFFs), as well as the \Nystrom method. From a theoretical perspective, we provide an upper bound on the value of $\Delta$ attained by LP-RFFs, showing there are important regimes in which these quantized features can be used to dramatically improve generalization performance under a memory budget. Empirically, across a number of tasks, we show that LP-RFFs consistently outperform FP-RFFs, which in term outperform the \Nystrom method, under fixed memory budgets. We explain these results through the theory of \citeauthor{avron17}; in particular, we measure the value of $\Delta$ attained by these methods, and show that the higher rank approximations systematically attain lower $\Delta$,\footnote{The primary exception is for very low-precision features, in which case the noise introduced by quantization affects the spectrum of $\tK$ and results in a larger $\Delta$.  See Theorem \avner{INSERT THEOREM \#}} and thus better generalization performance. This provides an important new perspective on the debate of whether FP-RFFs or \Nystrom is superior; while previous work argued that \Nystrom is superior because it attains better kernel approximation error and generalization performance for a fixed number of features \citep{nysvsrff12}, our work suggests that from a practical perspective, FP-RFFs are superior.  
%
%To summarize, our main contributions are as follows:
%
%% Jian suggestions
%% performance under memory budget
%% we observe performance correlated with $\Delta$
%% We propose LP-RFFs to extend rank, and prove generalization bounds 
%% We empirically validate that low-precision RFF can outperform low-precision RFF under memroy budget.
%
%\begin{enumerate}
%	\item We advocate for comparing kernel approximation methods in terms of their generalization performance under a memory budget.
%	\item We propose LP-RFFs for generating high-rank approximations under memory constraints.  We prove generalization bounds for these features, and show they outperform FP-RFFs and the \Nystrom method on a variety of tasks, under a memory budget.
%	\item We perform the first empirical comparison of generalization performance vs. $\Delta$, showing strong correlation across all the methods and datasets we tried.
%\end{enumerate}
%
%The rest of this paper is organized as follows: We review related work in Section \ref{sec:relwork}.  In Section \ref{sec:prelim} we provide background, including the generalization bounds of \citeauthor{avron17}\citep{avron17}.  In Section \ref{sec:lprff}, we introduce LP-RFFs, along with the corresponding theory.  We present our experiments in Section \ref{sec:experiments}, and conclude in Section \ref{sec:conclusion}.

%Lastly, we note that as far as we are aware, our work is the first to systematically measure $\Delta$ across various kernel approximation methods, and show it's empirical correlation with generalization performance.  We believe this to be of independent interest.

% it correlates very strongly with both the rank and generalization performance of the kernel approximation methods.

%: (1) rank correlates strongly with $\Delta$, and (2) $\Delta$ correlates strongly with generalization performance.  As a result, when considering a fixed memory budget, LP-RFFs outperform FP-RFFs, which in turn outperform the \Nystrom method; this is simply because LP-RFFs have the highest rank, and \Nystrom features the lowest, under a fixed budget. This provides an important new perspective on the debate of whether FP-RFFs or \Nystrom is superior; while previous work argued that \Nystrom is superior because it attains better kernel approximation error and generalization performance for a fixed number of features \citep{nysvsrff12}, our work suggests that from a practical perspective, FP-RFFs are superior.  
%Lastly, we note that as far as we are aware, our work is the first to systematically measure $\Delta$ across various kernel approximation methods, and show it's empirical correlation with generalization performance.  We believe this to be of independent interest.


%We study the theoretical and empirical performance of LP-RFFs, relative to full-precision RFFs, as well as the \Nystrom method.  Theoretically, we upper bound the $\Delta$ value attained by these quantized features, showing there are important regimes in which LP-RFFs can be used to dramatically improve generalization performance under a memory budget.  Empirically, we compare its performance to both FP-RFFs and the \Nystrom method \citep{nystrom}, for a wide-range of numbers of features $m$ and precisions $b$, showing that LP-RFFs can achieve dramatically lower approximation error than existing methods, under a fixed budget.  
%As part of this empirical analysis, we perform the first systematic study measuring $\Delta$ across these kernel approximation methods on real datasets.  We show that there is a very strong correlation between $\Delta$ and the generalization performance of these methods, across a number of different tasks (classification/regression), providing strong empirical support for the above-mentioned theory.  In addition to supporting the theory, this study reveals some key differences between the \Nystrom method and random Fourier features.  First of all, we show that although for a fixed number of features, \Nystrom achieves \emph{significantly} lower kernel approximation error, the values of $\Delta$ attained by these methods are similar.  An important consequence of this, given that the \Nystrom method requires more memory, is that RFFs consistently outperform the \Nystrom method, when considering a fixed memory budget. This provides an important new perspective on the debate of which method is superior; while previous work argued that \Nystrom is superior because it attains better kernel approximation error and generalization performance for a fixed number of features \citep{nysvsrff12}, our work suggests that from a practical perspective, RFFs are superior.

%To summarize, our main contributions are as follows:
%\begin{enumerate}
%	\item We propose using low-precision as a way to generate more features under the same memory budget.  We prove generalization bounds for these low-precision features, and empirically demonstrate the superior performance of these features to the \Nystrom method and full precision RFFs under memory constraints.
%	\item We perform the first empirical comparison of generalization performance vs. $\Delta$, showing strong correlation across all the different methods and datasets we tried.
%\end{enumerate}


%From a practical perspective, we believe using low-precision  This will require implementing LP-RFFs on fast hardware (\eg, GPUs, TPUs, FPGAs) that leverages low-precision for faster and more energy efficient computation.  