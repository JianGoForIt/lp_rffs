We investigate the problem of training kernel approximation models under memory constraints.  Our first step toward this goal is to compare the performance of two leading kernel approximation methods, the \Nystrom method \citep{nystrom} and random Fourier features (RFFs) \cite{rahimi07random}, in this memory constrained setting.
Although the \Nystrom method is generally known to outperform RFFs for a fixed number of features \citep{nysvsrff12}, we find that RFFs are able to outperform Nystrom features in this constrained setting.  This holds true in spite of the fact that the \Nystrom method often has smaller kernel approximation error than the RFF approximation, in terms of both the spectral norm and the Frobenius norm, under the same budget. 
In order to explain this phenomenon, we leverage the recent work of \citeauthor{avron17}\citep{avron17}; in this work, they consider an approximation $\tK$ to a kernel matrix $K$, which satisfies $(1-\Delta)(K+\lambda\id) \preceq \tK + \lambda\id \preceq (1+\Delta)(K+\lambda\id)$, and they bound the generalization performance of the regression model trained using $\tK$, with regularization constant $\lambda$, in terms of $\Delta$.  
We demonstrate across a number of tasks that this metric $\Delta$ correlates much more strongly with generalization performance than other measures of kernel approximation error, and show that under a fixed memory budget, random Fourier features systematically attains lower $\Delta$ values. We explain that this is due to the fact that the RFF approximation is higher rank than the \Nystrom approximation, when under the same memory budget.  Taking inspiration from these results, we propose using \emph{low-precision} random Fourier features (LP-RFFs) in order to generate an even higher rank approximation under the same memory budget. 
We build on the theory of \citeauthor{avron17} to analyze the generalization performance of LP-RFFs, proving that performance decays as \todo{...make formal statement}.
Empirically, we show the we are able to get within \todo{xx\%} of the performance of full-precision RFF models, using \todo{xx} times less memory during training, on the TIMIT speech recognition dataset.

%VERSION 2: Our story begins with an intriguing observation: across a variety of datasets and tasks, we consistently observe that random Fourier features (RFFs) \citep{rahimi07random} outperform the \Nystrom method \citep{nystrom} under a \emph{fixed memory budget}. This holds true in spite of the fact that the \Nystrom method often has smaller kernel approximation error than the RFF approximation, in terms of both the spectral norm and the Frobenius norm. In order to explain this phenomenon, we leverage the recent work of \citeauthor{avron17}\citep{avron17}; in this work, they consider an approximation $\tK$ to a kernel matrix $K$ satisfying $(1-\Delta)(K+\lambda\id) \preceq \tK + \lambda\id \preceq (1+\Delta)(K+\lambda\id)$, and bound the generalization performance of the model trained using $\tK$ in terms of $\Delta$. We demonstrate across a number of tasks that this metric $\Delta$ correlates much more strongly with generalization performance than other measures of kernel approximation error, and show that under a fixed memory budget, random Fourier features systematically attains lower $\Delta$ values. We explain that this is due to the fact that the RFF approximation is higher rank than the \Nystrom approximation, when under the same memory budget.  Taking inspiration from these results, we propose using \emph{low-precision} random Fourier features (LP-RFF) in order to generate an even higher-rank approximation under the same memory budget. We analyze the generalization performance of LP-RFF, proving that performance decays as \todo{...make formal statement}. Empirically, we validate the theory, and demonstrate that we are able to get within \todo{xx\%} of the performance of full-precision RFF models, using \todo{xx} times less memory during training.