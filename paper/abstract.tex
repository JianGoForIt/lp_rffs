% JENNIFER WIDOM
%State the problem, your approach and solution, and the main contributions of the paper. Include little if any background and motivation. Be factual but comprehensive. The material in the abstract should not be repeated later word for word in the paper.

Training performant kernel approximation models generally requires using a very large number of features; GPUs can be used for fast training, but have limited on-chip memory. To address this bottleneck, we investigate how to train kernel approximation models, which generalize well, \emph{under memory constraints}.  Building on recent theoretical work \citep{avron17,musco17}, we argue that generalization performance is intrinsically tied to the \emph{rank} of the approximate kernel matrix $\tK$. As a result, we propose using \emph{low-precision} random Fourier features (LP-RFFs) in order to generate a high-rank approximation, under a fixed memory budget. We prove a generalization bound for models trained on these low-precision features, showing there exist regimes in which using low precision does not degrade performance. Empirically, we demonstrate that LP-RFFs can match the generalization performance of both full-precision RFFs and the \Nystrom method, while using 4x-30x and 50x-435x less memory, respectively.
