% JENNIFER WIDOM
%State the problem, your approach and solution, and the main contributions of the paper. Include little if any background and motivation. Be factual but comprehensive. The material in the abstract should not be repeated later word for word in the paper.
\noindent
Training high-performance kernel approximation models often requires using a large number of features; as a result, the training bottleneck is frequently the amount of memory needed to store these features. This leads us to investigate how to train kernel approximation models that generalize well under memory constraints. We build on recent theoretical work to define a notion of relative spectral distance between a kernel matrix $K$ and its approximation $\tilde{K}$, and bound the generalization performance in terms of this distance. An important property of this metric is that for $\tilde{K}$ to be close to $K$, it must have at least a certain rank, regardless of the approximation method used. Thus, to create a high-rank approximation under a fixed memory budget, we propose using \emph{low-precision} random Fourier features (LP-RFFs); this allows us to capture more of the spectrum in a fixed amount of memory. We prove a generalization bound for kernel ridge regression models trained on these low-precision features, showing that lowering precision has negligible effect on performance when the quantization noise is small relative to the regularization parameter. Empirically, we demonstrate that LP-RFFs can match the generalization performance of both full-precision random Fourier features and the \Nystrom method, while using 5x-10x and 50x-460x less memory.


%While much of the kernel approximation literature focuses on bounding the reconstruction error between a kernel matrix $K$ and its approximation $\tK$, we argue that a more refined notion of distance is needed, when the goal is a tight bound on generalization performance.  