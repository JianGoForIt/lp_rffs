% JENNIFER WIDOM
%State the problem, your approach and solution, and the main contributions of the paper. Include little if any background and motivation. Be factual but comprehensive. The material in the abstract should not be repeated later word for word in the paper.

Training high-performance kernel approximation models generally requires using a large number of features; as a result, the training bottleneck is often the amount of memory needed to store these features. This leads us to investigate how to train kernel approximation models, which generalize well, under memory constraints. While much of the kernel approximation literature focuses on bounding the reconstruction error between a kernel matrix $K$ and its approximation $\tK$, we argue that a more refined notion of distance is needed, when the goal is a tight bound on generalization performance.  Toward this end, we build on recent theoretical work \citep{avron17,musco17} to define a notion of spectral distance between a kernel matrix $K$ and its approximation $\tK$. An important property of this metric is that for $\tK$ to be close to $K$, it must have at least a certain rank, regardless of the approximation method used. Thus, we propose using \emph{low-precision} random Fourier features (LP-RFFs) in order to generate a high-rank approximation, under a fixed memory budget. We prove a generalization bound for kernel ridge regression models trained on these low-precision features.  Empirically, we demonstrate that LP-RFFs can match the generalization performance of both full-precision random Fourier features and the \Nystrom method, while using 4x-30x and 50x-435x less memory, respectively.
