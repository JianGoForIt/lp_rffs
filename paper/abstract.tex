% JENNIFER WIDOM
%State the problem, your approach and solution, and the main contributions of the paper. Include little if any background and motivation. Be factual but comprehensive. The material in the abstract should not be repeated later word for word in the paper.

Training performant kernel approximation models generally requires using a very large number of features; GPUs can be used for fast training, but have limited on-chip memory. To address this bottleneck, we investigate the generalization performance of kernel approximation methods \emph{under memory constraints} for mini-batch based large scale training. Motivated by recent theoretical work \citep{avron17,musco17}, we argue that generalization performance is intrinsically tied to the \emph{rank} of the approximate kernel matrix $\tK$. Thus, we propose using \emph{low-precision} random Fourier features (LP-RFFs) in order to generate a high-rank approximation, under a fixed memory budget. We prove a generalization bound for kernel ridge regression trained on these low-precision features. Our theoretical analysis shows that there exist regimes, where low-precision has a negligible effect on the number of features required to achieve a certain guarantee on generalization performance. Empirically, we demonstrate that LP-RFFs can match the generalization performance of both full-precision RFFs and the \Nystrom method, while using 4x-30x and 50x-435x less memory, respectively.
