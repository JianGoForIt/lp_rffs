% JENNIFER WIDOM
%State the problem, your approach and solution, and the main contributions of the paper. Include little if any background and motivation. Be factual but comprehensive. The material in the abstract should not be repeated later word for word in the paper.

%In this paper we invt

%Our goal in this paper is to train kernel approximation models, which generalize well, 
%We investigate the problem of training kernel approximation models under memory constraints.

%Memory is often 

Training performant kernel approximation models generally requires using a very large number of features; GPUs can be used for fast training, but have limited on-chip memory. To address this bottleneck, we investigate how to train kernel approximation models, which generalize well, \emph{under memory constraints}.  Building on recent theoretical work \citep{avron17,musco17}, we argue that generalization performance is intrinsically tied to the \emph{rank} of the approximate kernel matrix $\tK$. As a result, we propose using \emph{low-precision} random Fourier features (LP-RFFs) in order to generate a high-rank approximation, under a fixed memory budget. We prove a generalization bound for models trained on these low-precision features, showing there exist regimes in which using lower precision does not degrade performance. Empirically, we compare the performance of LP-RFF with full-precision random Fourier features (FP-RFFs) \citep{rahimi07random}, as well as with the \Nystrom method \citep{nystrom}, and show it consistently outperforms both baselines \avner{Give concrete numbers here}. Our empirical analysis also reveals that FP-RFFs consistently outperforms the \Nystrom method in this memory constrained setting, challenging the prevailing belief that \Nystrom is better than FP-RFF \citep{nysvsrff12}.

%More specifically, we show that if $\tK$ satisfies $(1+\Delta)^(-1)(K+\lambda I) \leq \tK + \lambda I \leq (1+\Delta)(K+\lambda I)$, we can bound the generalization performance of a model trained using $\tK$ in place of $K$, in terms of $\Delta$; it can then be easily shown that achieving a value of $\Delta$ requires a number of features $r$ satisfying $\sigma_{r+1} \leq \lambda \Delta$.

%we are able to get within \todo{xx\%} of the performance of full-precision RFF models, using \todo{xx} times less memory during training, on the TIMIT speech recognition dataset.
%We also compare performance 
%Answering this question requires a deep understanding of what makes a kernel approximation method generalize; 


%In this paper, we compare the generalization performance two leading kernel approximation methods, the \Nystrom method \citep{nystrom} and random Fourier features (RFFs) \cite{rahimi07random}, under a fixed memory budget.  We show,  that RFFs outperform 

%We find that although the \Nystrom method is generally known to outperform RFFs for a fixed number of features \citep{nysvsrff12}, we find that RFFs are able to outperform Nystrom features in this constrained setting, in spite of often having larger kernel approximation error.





%VERSION 1: Our story begins with an intriguing observation: across a variety of datasets and tasks, we consistently observe that random Fourier features (RFFs) \citep{rahimi07random} outperform the \Nystrom method \citep{nystrom} under a \emph{fixed memory budget}. This holds true in spite of the fact that the \Nystrom method often has smaller kernel approximation error than the RFF approximation, in terms of both the spectral norm and the Frobenius norm. In order to explain this phenomenon, we leverage the recent work of \citeauthor{avron17}\citep{avron17}; in this work, they consider an approximation $\tK$ to a kernel matrix $K$ satisfying $(1-\Delta)(K+\lambda\id) \preceq \tK + \lambda\id \preceq (1+\Delta)(K+\lambda\id)$, and bound the generalization performance of the model trained using $\tK$ in terms of $\Delta$. We demonstrate across a number of tasks that this metric $\Delta$ correlates much more strongly with generalization performance than other measures of kernel approximation error, and show that under a fixed memory budget, random Fourier features systematically attains lower $\Delta$ values. We explain that this is due to the fact that the RFF approximation is higher rank than the \Nystrom approximation, when under the same memory budget.  Taking inspiration from these results, we propose using \emph{low-precision} random Fourier features (LP-RFF) in order to generate an even higher-rank approximation under the same memory budget. We analyze the generalization performance of LP-RFF, proving that performance decays as \todo{...make formal statement}. Empirically, we validate the theory, and demonstrate that we are able to get within \todo{xx\%} of the performance of full-precision RFF models, using \todo{xx} times less memory during training.

% VERSION 2: We investigate the problem of training kernel approximation models under memory constraints.  Our first step toward this goal is to compare the performance of two leading kernel approximation methods, the \Nystrom method \citep{nystrom} and random Fourier features (RFFs) \cite{rahimi07random}, in this memory constrained setting. Although the \Nystrom method is generally known to outperform RFFs for a fixed number of features \citep{nysvsrff12}, we find that RFFs are able to outperform Nystrom features in this constrained setting.  This holds true in spite of the fact that the \Nystrom method often has smaller kernel approximation error than the RFF approximation, in terms of both the spectral norm and the Frobenius norm, under the same budget.  In order to explain this phenomenon, we leverage the recent work of \citeauthor{avron17}\citep{avron17}; in this work, they consider an approximation $\tK$ to a kernel matrix $K$, which satisfies $(1-\Delta)(K+\lambda\id) \preceq \tK + \lambda\id \preceq (1+\Delta)(K+\lambda\id)$, and they bound the generalization performance of the regression model trained using $\tK$, with regularization constant $\lambda$, in terms of $\Delta$. We demonstrate across a number of tasks that this metric $\Delta$ correlates much more strongly with generalization performance than other measures of kernel approximation error, and show that under a fixed memory budget, random Fourier features systematically attains lower $\Delta$ values. We explain that this is due to the fact that the RFF approximation is higher rank than the \Nystrom approximation, when under the same memory budget.  Taking inspiration from these results, we propose using \emph{low-precision} random Fourier features (LP-RFFs) in order to generate an even higher rank approximation under the same memory budget. We build on the theory of \citeauthor{avron17} to analyze the generalization performance of LP-RFFs, proving that performance decays as \todo{...make formal statement}. Empirically, we show the we are able to get within \todo{xx\%} of the performance of full-precision RFF models, using \todo{xx} times less memory during training, on the TIMIT speech recognition dataset.
