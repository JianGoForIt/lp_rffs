For an extended discussion of the related work, see Appendix \ref{sec:appendix_blob}. We provide a brief overview here:
\paragraph{Low-memory kernel approximation} For RFFs, there has been work on using structured random projections \citep{fastfood,yu15,sphereRKS}, and feature selection \citep{sparseRKS, may2016} to reduce memory utilization. Our work is orthogonal to these, because LP-RFFs can be used in conjunctions with both. For \NystromNS, there has been extensive work on improving the choice of landmark points, and reducing the memory footprint in other ways \cite{ensemble09,fastpred14,meka14}. In our work, we specifically focus on the effect of \textit{quantization} on kernel approximation performance per bit, and note that RFFs are much more amenable to quantization. For a more extended discussion on quantization and \NystromNS, see Appendix~\ref{sec:appendix_blob}.
%\paragraph{Large-scale kernel methods} The most relevant works are \citet{block16,may2017}, and we believe both of these directions could benefit from lower footprint training.
\paragraph{Low-precision} There has been much recent interest in the topic of low-precision for accelerating training and/or inference, as well as for compressing models \citep{gupta15,hogwild15,hubara16,halp18,desa17,han15}.  There have been many advances in hardware support for low-precision as well\citep{tpu17,brainwave17}.
\paragraph{\Nystrom vs. RFF} Our work responds to the claim in \citet{nysvsrff12} that \Nystrom is categorically better than RFFs by showing that when memory is considered, the opposite is true.

%
%\begin{itemize}
%	\item \textbf{Low-memory kernel approximation}: There are several related lines of work.  (1) Structured matrices for RFF computation\citep{fastfood,yu15,sphereRKS}, (2) Feature selection for RFFs\citep{sparseRKS, may2016}, (3) improved landmark point selection for \Nystrom \citep{kmeans08,kumar12,gittens13}, (4) Low-memory footprint \Nystrom approximation \cite{ensemble09,fastpred14,meka14}. Our work is orthogonal to 1 and 2 because LP-RFFs can be used in conjunction with these methods. Our work focus on RFFs, and is thus separate from the \Nystrom work. 
%	\item \textbf{Large-scale kernel methods}: The most relevant works are \citet{block16,may2017}, and we believe both of these directions could benefit from lower footprint training.
%	\item \textbf{Low-precision}:  There has been much recent interest in the topic of low-precision for accelerating training and/or inference, as well as for compressing models \citep{gupta15,hogwild15,hubara16,halp18,desa17,han15}.  There have been many advances in hardware support for low-precision as well\citep{tpu17,brainwave17}.
%	\item \textbf{\Nystrom vs. RFF}: Our work responds to the claim in \citet{nysvsrff12} that \Nystrom is categorically better than RFFs by showing that when memory is considered, the opposite is true.
%\end{itemize}
%
