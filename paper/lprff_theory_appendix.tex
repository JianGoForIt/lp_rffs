Let $K \in \mathbb{R}^{n \times n}$ be the kernel matrix.
Let $Z \in \mathbb{R}^{n \times m}$ be the random Fourier feature matrix, where $n$ is
the number of data points and $m$ is the number of features.
We can write $Z = \frac{1}{\sqrt{m}} \begin{bmatrix} z_1, \dots,
  z_m \end{bmatrix}$ where $z_i$ are the (scaled) columns of $Z$.
Each entry of $Z$ has the form $\sqrt{2} \cos(w^T x + b)$ for some $w, x \in
\mathbb{R}^{d}$ and $b \in \mathbb{R}^{m}$, where $d$ is the dimension of the
original dataset.
Then $\E[z_i z_i^\top] = K$, so $\E [Z Z^\top] = K$ as we have discussed.

Now suppose we quantize $Z$ to $b$ bits, for some fixed $b \geq 1$.
Then the quantized feature matrix is $Z + C$ for some random $C \in \mathbb{R}^{n
  \times m}$ whose entries are independent conditioned on $Z$ (but not identically
distributed) with $\E[C \mid Z] = 0$.
We can write $C = \frac{1}{\sqrt{m}} \begin{bmatrix} c_1, \dots,
  c_m \end{bmatrix}$ where $c_i$ are the (scaled) columns of $C$.
Moreover, the $c_i$ are independent conditioned on $Z$.
We also showed that the entries $C_{ij}$ have variance $\E[C_{ij}^2 \mid Z] \leq \frac{2}{(2^b - 1)^2}$.
Call this variance $\delta^2_b \defeq \frac{2}{(2^b - 1)^2}$.

We first analyze the expectation of $(Z + C) (Z + C)^\top$ (over both the
randomness of $Z$ and of $C$).
\begin{proposition}
  $\E[(Z + C)(Z + C)^\top] = K + D$, where $D \defeq \E[C C^\top]$ is a
  diagonal matrix satisfying $0 \preceq D \preceq \delta^2_b I_n$.
  \label{prop:expectation_CCstar}
\end{proposition}

\begin{proof}
  Since $\E[C \mid Z] = 0$,
  \begin{equation*}
    \E[(Z + C) (Z + C)^\top \mid Z] = Z Z^\top + \E[C C^\top \mid Z].
  \end{equation*}
  We see that $\E[C C^\top \mid Z] = \frac{1}{m} \sum_{i=1}^{m} \E[c_i c_i^\top \mid Z]$.
  For each $i$, $\E[c_i c_i^\top \mid Z] = \diag(\E[(c_i)_1^2 \mid Z], \dots,
  \E[(c_i)_n^2 \mid Z])$.
  We already showed that $\E[(c_i)_j^2 \mid Z] \leq \delta^2_b$, so $\E[C C^\top \mid Z] \preceq \delta^2_b I_n$.
  Taking expectation wrt $Z$ yields
  \begin{equation*}
    \E[(Z + C)(Z + C)^\top] = \E[Z Z^\top] + \E[\E[C C^\top \mid Z]] = K + D,
  \end{equation*}
  where $D \defeq \E[C C^\top]$ is diagonal and $0 \preceq D \preceq \delta^2_b I_n$.
\end{proof}

With $D \defeq \E[C C^\top]$, we can use matrix concentration to show that the
quantized kernel matrix $K_\mathrm{lp} \defeq (Z + C)(Z + C)^\top$ is close to its
expectation $K + D$.
There are a couple of ways to state this, depending on which basis we want the
bound to hold.
To change the basis, we conjugate by a matrix $B \in \mathbb{R}^{n \times n}$.
Our goal is to show that $\norm{B (Z + C)(Z + C)^\top B^\top - B(K + D)B^\top}$ is small
with high probability.
The matrices we will use to conjugate will be $B = (K + \lambda I_n)^{-1/2}$, but the
following proposition holds for any $B$.

\begin{proposition}
  Let $L \defeq 2n \norm{B}^2$ and $M \defeq B(K + \delta^2_b I_n) B^\top$,
  then for any $t \geq \sqrt{\norm{M} / m} + 2L/3m$,
  \begin{equation*}
    P(\norm{B(Z + C)(Z + C)^\top B - B(K + D)B} \geq t) \leq \frac{8\tr(M)}{\norm{M}}
    \exp \left( \frac{-mt^2}{2L(\norm{M} + 2t/3)} \right).
  \end{equation*}
  \label{prop:quantized_concentration}
\end{proposition}
Concentration is exponential in the number of features $m$, as expected.

\begin{proof}
  Let $S_i = \frac{1}{m} \left( B (z_i + c_i) (z_i + c_i)^\top B^\top - B (K + D)B^\top
  \right)$ and $S = \sum_{i=1}^{m} S_i$.
  We see that $\E[S_i] = 0$.
  We will bound $\norm{S}$ by applying the matrix Bernstein inequality.
  Thus we need to bound $\norm{S_i}$ and $\norm{\sum_{i=1}^{m} \E[S_i^2]}$.

  Let $v_i = B(z_i + c_i) \in \mathbb{R}^{n}$, then $S_i = \frac{1}{m} (v_i
  v_i^\top  - \E[v_i v_i^\top])$.
  We first bound $\norm{v_i v_i^\top}$.
  Since this is a rank 1 matrix,
  \begin{equation*}
    \norm{v_i v_i^\top} = \norm{v_i}^2 = \norm{B(z_i + c_i)}^2 \leq \norm{B}^2
    \norm{z_i + c_i}^2 \leq 2n \norm{B}^2,
  \end{equation*}
  where we have used the fact that $z_i + c_i$ is a vector of length $n$ whose
  entries are in $[-\sqrt{2}, \sqrt{2}]$.
  This gives a bound on $\norm{S_i}$:
  \begin{equation*}
    \norm{S_i} = \frac{1}{m} \norm{v_i v_i^\top - \E[v_i v_i^\top]}
    \leq \frac{1}{m} \norm{v_i v_i^\top} + \frac{1}{m} \E \norm{v_i v_i^\top}
    \leq \frac{4n \norm{B}^2}{m} = 2L/m.
  \end{equation*}

  Now it's time to bound $\E[S_i^2]$.
  \begin{equation*}
    \E[S_i^2] = \frac{1}{m^2} \var[v_i v_i^\top] \preceq \frac{1}{m^2} \E[(v_i v_i^\top)^2]
    = \frac{1}{m^2} \E[v_i v_i^\top v_i v_i^\top] = \frac{1}{m^2} \E[\norm{v_i}^2 v_i
    v_i^\top] \preceq \frac{2n \norm{B}^2}{m^2} \E[v_i v_i^\top].
  \end{equation*}
  Thus
  \begin{equation*}
    \sum_{i=1}^{m} \E[S_i^2] \preceq \frac{2n \norm{B}^2}{m} \E[v_i v_i^\top] = \frac{2n
      \norm{B}^2}{m} B(K + D) B^\top \preceq \frac{2n \norm{B}^2}{m} B(K + \delta^2_b I_n)B^\top
    = LM/m.
  \end{equation*}

  Applying the matrix Bernstein inequality (with intrinsic dimension) (Theorem
  7.3.1 in \cite{tropp2015introduction}), for $t \geq \sqrt{L \norm{M}/m} + 2L/3m$,
  \begin{equation*}
    P(\norm{B(Z + C)(Z + C)^\top B - B(K + D)B} \geq t) \leq \frac{8\tr(M)}{\norm{M}}
    \exp \left( \frac{-t^2/2}{L \norm{M}/m + 2Lt/3m} \right).
  \end{equation*}
\end{proof}

Now we are ready to show that low precision features yield close spectral
approximation to the kernel matrix.

\begin{theorem}
  Suppose that $\tK = (Z+C)(Z+C)^\top$, $\norm{K} \geq \lambda$ and $\delta^2_b \leq \lambda$.
  Then for any $\Delta \leq 1/2$,
  \begin{equation*}
  %(1 + \Delta)^{-1}(K + \lambda I_n) \preceq (Z + C) (Z + C)^\top + \lambda I_n \preceq (1 + \Delta)(K + \lambda I_n)
    \Prob\Big[D_{\lambda}(K,\tK) \leq \Delta\Big] \geq 1 - 16 \tr((K +
    \lambda I_n)^{-1} (K + \delta^2_b I_n)) \exp \left( -\frac{9m \big(\frac{2}{3}\Delta - \delta^2_b / \lambda\big)^2}{44n/\lambda} \right).
  \end{equation*}
  Thus if we use $m \geq \frac{44}{9 \big(\frac{2}{3}\Delta - \delta^2_b / \lambda\big)^2} n/\lambda \log (16 \tr((K + \lambda I_n)^{-1} (K +
  \delta^2_b I_n)) / \rho)$
  features, then $D_\lambda(K,\tK)\leq \Delta$  with probability at least $1 - \rho$.
\end{theorem}
%$\Delta_0 \in \Big[\sqrt{\frac{2n/\lambda}{m}} + \frac{4n/\lambda}{3m}, \frac{1}{3} - \frac{\delta^2_b}{\lambda}\Big]$, with

%\Delta = \frac{3}{2}\Big(\Delta_0 + \delta^2_b / \lambda\Big)
% \frac{2}{3}\Delta = \Delta_0 + \delta^2_b / \lambda
% \Delta_0  = \frac{2}{3}\Delta - \delta^2_b / \lambda

\begin{proof}
First, we use the fact that for any $x\in[0,1/2]$, $(1+x)^{-1} \leq 1-\frac{2}{3}x$, and note the following:
\begin{eqnarray*}
\Prob\Big[D_{\lambda}(K,\tK) \leq \Delta\Big] &=& \Prob\Big[(1 + \Delta)^{-1}(K + \lambda I_n) \preceq (Z + C) (Z + C)^\top + \lambda I_n \preceq (1 + \Delta)(K + \lambda I_n)\Big] \\
&\geq& \Prob\Big[(1 - \frac{2}{3}\Delta)(K + \lambda I_n) \preceq (Z + C) (Z + C)^\top + \lambda I_n \preceq (1 + \frac{2}{3}\Delta)(K + \lambda I_n)\Big]\\
&=& \Prob\Big[(1 - \Delta')(K + \lambda I_n) \preceq (Z + C) (Z + C)^\top + \lambda I_n \preceq (1 + \Delta')(K + \lambda I_n)\Big],
\end{eqnarray*}
where we let $\Delta' \defeq \frac{2}{3}\Delta$.  We now focus on lower bounding this final expression.

Consider the condition $(1 - \Delta')(K + \lambda I_n) \preceq (Z + C) (Z + C)^\top + \lambda I_n \preceq (1 +
\Delta')(K + \lambda I_n)$.
Let $B \defeq (K + \lambda I_n)^{-1/2}$, which is symmetric, then $\norm{B}^2 =
\norm{K + \lambda I_n}^{-1} \leq 1/\lambda$.
Conjugating by $B \defeq (K + \lambda I_n)^{-1/2}$ (i.e.\ multiplying by $B$ on the
left and right) yields an equivalent condition (as $B$ is
invertible):
\begin{equation*}
(1 - \Delta') I_n \preceq (K + \lambda I_n)^{-1/2} ((Z + C) (Z + C)^\top + \lambda I_n) (K + \lambda I_n)^{-1/2} \preceq (1 + \Delta') I_n.
\end{equation*}
Subtracting $I_n$ from both sides:
\begin{equation*}
-\Delta' I_n \preceq (K + \lambda I_n)^{-1/2} ((Z + C) (Z + C)^\top + \lambda I_n - (K + \lambda I_n)) (K + \lambda I_n)^{-1/2} \preceq \Delta' I_n.
\end{equation*}
This is equivalent to
\begin{equation*}
\norm{(K + \lambda I_n)^{-1/2} ((Z + C) (Z + C)^\top - K) (K + \lambda I_n)^{-1/2}} \leq \Delta'.
\end{equation*}
Suppose that there is some $\Delta_0 > 0$ such that $\norm{B((Z + C)(Z + C)^\top - (K + D))B} \leq \Delta_0$.
Then since $0 \preceq D \preceq \delta^2_b I_n$, $\norm{B(K + D - K) B} = \norm{B D B} \leq \delta^2_b
\norm{B}^2$, and the triangle inequality yields
\begin{align*}
\norm{B((Z + C)(Z + C)^\top - K)B}
&\leq \norm{B((Z + C)(Z + C)^\top - (K + D))B} + \norm{B (K + D - K)B} \\
&\leq \Delta_0 + \delta^2_b \norm{B}^2 \\
&\leq \Delta_0 + \delta^2_b/\lambda.
\end{align*}
We can now let $\Delta' = \Delta_0 + \delta^2_b/\lambda$.  Given that $\Delta' = \frac{2}{3}\Delta$, and
$\Delta \leq 1/2$, this implies that $\Delta_0 \leq 1/3$.

Now, it suffices to show that
\begin{equation*}
P(\norm{B((Z + C)(Z + C)^\top - (K + D))B} \geq \Delta_0) \leq 8 \tr((K + \lambda I_n)^{-1} (K +
\delta^2_b I)) \exp \left( -\frac{9m \Delta_0^2}{44n/\lambda} \right).
\end{equation*}

We have $L \defeq 2n \norm{B}^2 \leq 2n/\lambda$.
Let $\lambda_1, \dots, \lambda_n$ be the eigenvalues of $K$.
We have
\begin{equation*}
M \defeq B(K + \delta^2_b I_n) B =  (K + \lambda I_n)^{-1/2} (K + \delta^2_b I_n) (K + \lambda
I_n)^{-1/2} = \diag((\lambda_1 + \delta^2_b)/(\lambda_1 + \lambda), \dots,
(\lambda_n + \delta^2_b) / (\lambda_n + \lambda)).
\end{equation*}
By assumption, $\norm{K} \geq \lambda$, so $\norm{M} = \frac{\lambda_1 + \delta^2_b}{\lambda_1 + \lambda} \geq
1/2$.
We also assume that $\delta^2_b \leq \lambda$, so $\norm{M} \leq 1$.
Moreover, $\tr(M) = \tr((K + \lambda I_n)^{-1/2} (K + \delta^2_b I) (K + \lambda I_n)^{-1/2}) =
\tr((K + \lambda I_n)^{-1} (K + \delta^2_b I_n))$.
The condition of $\Delta_0 \geq \sqrt{L \norm{M} / m} + 2L/3m$ becomes $\Delta_0 \geq \sqrt{\frac{2n/\lambda}{m}} + \frac{4n/\lambda}{3m}$.
The bound of Proposition~\ref{prop:quantized_concentration} becomes:
\begin{align*}
&P(\norm{(K + \lambda I_n)^{-1/2} ((Z + C) (Z + C)^\top - (K + D)) (K + \lambda
  I_n)^{-1/2}} \geq \Delta_0) \\
\leq&\ \frac{8 \tr((K + \lambda I_n)^{-1} (K + \delta^2_b I_n))}{1/2} \exp \left( -\frac{m
  \Delta_0^2}{4\frac{n}{\lambda} (1 + 2\Delta_0/3)} \right) \\
\leq&\ 16 \tr((K + \lambda I_n)^{-1} (K + \delta^2_b I_n)) \exp \left( -\frac{9m\Delta_0^2}{44n/\lambda} \right)
\numberthis \label{eq:concentration_bound}
\end{align*}
where we get the last inequality from $\Delta_0 \leq 1/3$.

In the case where $\Delta_0 \leq \sqrt{\frac{2n/\lambda}{m}} + \frac{4n/\lambda}{3m}$, the bound is
vacuous (greater than 1).
Indeed, since $\Delta_0 \leq 1/2$, we can assume $\sqrt{\frac{2n/\lambda}{m}} \leq 1/2$, so
$\frac{4n/\lambda}{3m} = \frac{2}{3} \frac{2n/\lambda}{m} \leq \frac{2}{3} \frac{1}{2}
\sqrt{\frac{2n/\lambda}{m}} = 1/3 \sqrt{\frac{2n/\lambda}{m}}$.
Hence we have $\Delta_0 \leq 4/3 \sqrt{\frac{2n/\lambda}{m}}$, and so
\begin{equation*}
  16 \exp \left( -\frac{9m \Delta_0^2}{44 n/\lambda} \right) \geq 16 \exp \left( -\frac{9 \cdot
      16/9 \cdot 2n/\lambda}{44 n/\lambda} \right) = 16 \exp(-8/11) \geq 7.
\end{equation*}
Since we assume that $\norm{K} \geq \lambda$,
\begin{equation*}
  \tr ((K + \lambda I_n)^{-1} (K + \delta^2_b I_n)) \geq \frac{\lambda_1 + \delta^2_b}{\lambda_1 + \lambda} \geq \frac{1}{2}.
\end{equation*}
Thus the bound in equation~\eqref{eq:concentration_bound} is at least $7/2$,
which is vacuous.
Hence we do not need to include the condition $\Delta_0 \geq \sqrt{\frac{2n/\lambda}{m}} +
\frac{4n/\lambda}{3m}$,
and the bound~\eqref{eq:concentration_bound} holds for all $\Delta_0 \leq 1/2$.

Letting this probability be $\rho$ and solving for $m$ yields
\begin{eqnarray*}
m &\geq& \frac{44}{9 \Delta_0^2} n/\lambda \log (16 \tr((K + \lambda I_n)^{-1} (K + \delta^2_b I_n)) / \rho) \\
&=& \frac{44}{9 \Big((2/3)\Delta - \delta_b^2/\lambda\Big)^2} n/\lambda \log (16 \tr((K + \lambda I_n)^{-1} (K + \delta^2_b I_n)) / \rho).
\end{eqnarray*}

\end{proof}

There is a bias--variance trade-off: as we decrease the number of bits $b$, under
a fixed memory budget, we can use more features, and $(Z + C)(Z + C)^\top$
concentrates more strongly (lower variance) around the expectation $K + D$ with
$0 \preceq D \preceq \delta^2_b I_n$, but this expectation is further away from the true kernel
matrix $K$ (larger bias).
Thus there should be an optimal number of bits $b^*$ that balances the bias and
the variance.

If we let the number of bits $b$ goes to $\infty$, we recover the
result of \citet{avron17}.
\begin{corollary}
  Suppose that $\tK = ZZ^\top$, $\norm{K} \geq \lambda$.
Then for any $\Delta \leq 1/2$,
\begin{equation*}
%(1 + \Delta)^{-1}(K + \lambda I_n) \preceq (Z + C) (Z + C)^\top + \lambda I_n \preceq (1 + \Delta)(K + \lambda I_n)
\Prob\Big[D_{\lambda}(K,\tK) \leq \Delta\Big] \geq 1 - 16 \tr((K +
\lambda I_n)^{-1} (K + \delta^2_b I_n)) \exp \left( -\frac{m \Delta^2}{11n/\lambda} \right).
\end{equation*}
Thus if we use $m \geq \frac{11}{\Delta^2} n/\lambda \log (16 \tr((K + \lambda I_n)^{-1} (K + \delta^2_b I_n)) / \rho)$
features, then $D_\lambda(K,\tK)\leq \Delta$  with probability at least $1 - \rho$.
\end{corollary}
The constants are slightly different from that of \citet{avron17} as we use the
real features $\sqrt{2} \cos(w^T x + b)$ instead of the complex features $\exp(i
w^T x)$, and our definition of $\Delta$-spectral approximation is slightly different
from theirs.

The number of features depend linearly on $n/ \lambda$.
\citet{avron17} provided a lower bound, showing that the number of random Fourier features
must depend linearly on $n / \lambda$.
%For optimal minimax rate, the value of $\lambda$ is of order $\sqrt{n}$ (\todo{check this}), so the number of features is still sublinear in $n$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "nips_2018"
%%% End:
