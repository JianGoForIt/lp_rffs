\subsection{Background: Kernel approximation}
\begin{itemize}
	\item Give a very brief overview of kernel approximation, Nystrom and RFF.
	\item Discuss the amount of memory each of these methods uses during training.  It is important to be very clear about how we are measuring memory.
\end{itemize}

\subsection{Avron Generalization Bound}
\begin{itemize}
	\item Present their bound, and discuss in moderate detail. 
	$$R(\tf) \leq \frac{1}{1-\Delta}\hat{R}_K(f) + \frac{\Delta}{1+\Delta} \cdot \frac{rank(\tK)}{n}\cdot \sigma_{\nu}^2$$ 
	\item Mention that we can understand $\Delta$ in terms of $D =\max_i |\log(\sigma_i + \lambda) - \log(\tsigma_i + \lambda)|$ ($1+\Delta \geq \exp(D)$), as a way of visualizing the bound; if you plot both spectra $+\lambda$ in log-scale, the $\Delta$ is the maximum gap between the two lines (if eigenvectors are the same.  So even if the eigenvectors are different, this us gives lower-bound for the true value of $\Delta$).
	\item Discuss their result which bounds the number of random Fourier features needed to get a specific $\Delta$ value.
\end{itemize}

\subsection{Notation}
\begin{itemize}
	\item Present notation for Nystrom and RFF features, kernel matrices, etc.  
	\item Specify that $n$ denotes number of datapoints, $d$ is data dimension, $m$ is number of random features, etc.
\end{itemize}