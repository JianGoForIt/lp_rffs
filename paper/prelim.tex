We now discuss the way we measure the memory utilization of these kernel approximation methods, and review generalization bounds in the context of kernel ridge regression.  For background on kernel approximation methods, or for an overview of the notation we use in this paper, see Appendix \ref{sec:appendix_blob}.

\subsection{Memory utilization}
\label{subsec:memory_utils}
The core resource we optimize for in this work is memory.  As discussed in the introduction, we consider the setting of large-scale mini-batch training for kernel approximation methods. We summarize the memory utilization of the different parts of the training pipeline in Table \ref{table:mem-usage}, assuming full-precision numbers are stored in 32 bits. The three components are:
\begin{enumerate}
	\item \textit{Feature generation}: Computing RFFs, for example, requires storing the random projection matrix $W \in \RR^{m\times d}$.
	\item \textit{Mini-batch storage}: The vectors $z(x_i)\in\RR^m$ for all $x_i$ in the current mini-batch, must be stored.
	\item \textit{Learned parameters}: For binary classification and regression, the linear model learned on the $z(x)$ features is a vector $\theta \in \RR^m$; for $c$-class classification, it is a matrix $\theta \in \RR^{m\times c}$.	
\end{enumerate}

\begin{table}
	\caption{Kernel approximation method memory utilization. $m$ is the number of kernel approximation features, $d$ is the dimensionality of a datum, $s$ is the mini-batch size, $c$ is the number of classes ($c=1$ for regression/binary classification), $b$ is the precision used for the random features, and $\hb$ is the precision used for the model.}
	\label{table:mem-usage}
	\centering
	\begin{tabular}{llll}
		\toprule
		Approximation Method & Feature generation & Mini-batch storage & Learned parameters \\
		\midrule
		\Nystrom \citep{nystrom} & $32(md + m^2)$ & $32ms$ & $32mc$ \\
		RFFs \citep{rahimi07random} &  $32md$ & $32ms$ & $32mc$ \\
		Circulant RFFs \citep{yu15} & $32m$ & $32ms$ & $32mc$ \\
		Low-precision RFFs (ours)& $32m$ & $bms$ & $\hb mc$ \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Fixed design kernel ridge regression generalization bounds}
\label{sec:genbound}
%In fixed design linear regression, one is given a set of labeled points $\{(x_i,y_i)\}_{i=1}^n$, where $x_i \in \RR^d$, $y_i = \by_i + \eps_i \in \RR$, and the $\eps_i$ are uncorrelated random variables with shared variance $\sigma^2 > 0$.  Given such a sample, the goal is to learn a regressor $f(x) = \beta^T x$ such that $\cR(f) = \expect{\eps}{\frac{1}{n}\sum_{i=1}^n (f(x_i) - \by_i)^2}$ is small. Note that for a fixed learning method the learned regressor $f$ seen as a random function, based on the random label noise $\eps_i$.
%One approach to solving this problem is kernel ridge regression (KRR).  In kernel ridge regression, one chooses a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, and a regularizing constant $\lambda$, and learns a function of the form $f(x) = \sum_i \alpha_i k(x,x_i)$.  Letting $K\in\RR^{n\times n}$ denote the kernel matrix such that $K_{ij} = k(x_i,x_j)$, and $y = (y_1,\ldots,y_n)$, the closed form solution for this problem is $\alpha = (K+\lambda I)^{-1}y$. It can then be shown that the expected error of this regressor $f_K$, under the fixed design setting, is:

In fixed design kernel ridge regression, one is given a set of labeled points $\{(x_i,y_i)\}_{i=1}^n$, where $x_i \in \RR^d$, $y_i = \by_i + \eps_i \in \RR$, and the $\eps_i$ are uncorrelated random variables with shared variance $\sigma^2 > 0$. Given such a sample, and a kernel function $k:\RR^d\times\RR^d\rightarrow \RR$, the goal is to learn a regressor $f(x) = \sum_i \alpha_i k(x,x_i)$ such that the expected error $\cR(f) = \expect{\eps}{\frac{1}{n}\sum_{i=1}^n (f(x_i) - \by_i)^2}$ is small. Given a regularization parameter $\lambda$, and letting $K\in\RR^{n\times n}$ denote the kernel matrix, and $y = (y_1,\ldots,y_n)$, the closed form solution for this problem is $\alpha = (K+\lambda I)^{-1}y$. It can then be shown that the expected error of this regressor $f_K$, under the fixed design setting, is:

\begin{eqnarray}
\cR(f_K) = \frac{1}{n}\lambda^2 \by^T(K+\lambda I)^{-2}\by + \frac{1}{n}\sigma^2 Tr\Big(K^2(K+\lambda \id)^{-2}\Big),
\end{eqnarray}
where $\by = (\by_1,\ldots,\by_n)$ is the vector of ``noiseless labels.'' See Appendix \todo{X} for the derivation.

We proceed to bound the expected loss $\cR(f_{\tK})$ of a kernel ridge regression model $f_{\tK}$ learned using an approximate kernel matrix $\tK$, in place of the exact kernel matrix $K$. First, we define the following notion of distance\footnote{Importantly, this isn't a properly defined ``distance function'' in the formal sense.  In particular, it is non-negative and symmetric, but only satisfies a weaker version of the triangle inequality: $D_{\lambda}(K_1,K_3) \leq D_{\lambda}(K_1,K_2) + D_{\lambda}(K_2,K_3) + D_{\lambda}(K_1,K_2)D_{\lambda}(K_2,K_3)$.} between $K$ and $\tK$:

\begin{definition}
	\label{def:specdist}
	Given a kernel matrix $K$, an approximation to it $\tK$, and a scalar $\lambda \geq 0$, we define the \textbf{relative spectral distance} $D_{\lambda}(K,\tK)\geq 0$ between $K$ and $\tK$ as:
	$$D_{\lambda}(K,\tK) = \min \Big\{\Delta \in \RR_{+} \;\;\Big|\;\; \frac{1}{1+\Delta}(K+\lambda I) \preceq \tK + \lambda I \preceq (1+\Delta)(K+\lambda I)\Big\}.$$
\end{definition}

We note that this definition is tightly related to the notion of $\Delta$-spectral approximation from \citet{avron17}.  For a discussion of the differences between our definition and theirs, please see Appendix~\ref{sec:appendix_blob}. Following \citet{avron17}, if we then define 
$$\hcR(f_K) = \frac{1}{n}\lambda \by^T(K+\lambda I)^{-1}\by + \frac{1}{n}\sigma^2 Tr\Big(K(K+\lambda)^{-1}\Big) \geq \cR(f_K),$$ 
we can bound the expected loss of $f_{\tK}$ as follows:

\begin{proposition}{Adapted from \citep{avron17}:}
	\label{prop:avron}
	Suppose $\tK$ is an approximation to a kernel matrix $K$, and $f_{K}$ and $f_{\tK}$ are the KRR estimators learned using these matrices, with regularizing constant $\lambda$ and label noise variance $\sigma^2$. Then the following bound holds:
	\begin{eqnarray}
	\cR(f_{\tK}) \leq \Big(1+D_{\lambda}(K,\tK)\Big)\cdot \hcR(f_K) + \frac{D_{\lambda}(K,\tK)}{1+D_{\lambda}(K,\tK)}\cdot \frac{rank(\tK)}{n}\cdot\sigma^2.
	\end{eqnarray}
\end{proposition}
We include a proof in Appendix \ref{sec:lprff_theory_appendix}. \avner{Include proof.}

An important consequence of the above definitions, is that if $\tK$ is a rank $r$ matrix, in order for $D_{\lambda}(K,\tK) \leq \Delta$, it must hold that $\lambda_{r+1}(K) \leq \Delta \lambda$, where $\lambda_i(K)$ denote the $i^{th}$ largest eigenvalues of $A$.  This follows from $A\preceq B \Rightarrow \lambda_i(A) \leq \lambda_i(B)$, $(\lambda_i(K) + \lambda)/(1+\Delta) \leq \lambda_i(\tK) + \lambda$, and $\lambda_{r+1}(\tK) = 0$.  This sets a lower bound on the rank necessary for $D_{\lambda}(K,\tK) \leq \Delta$, which holds regardless of the approximation method used. This insight motivates our proposed low-precision random Fourier features method, which generates high-rank kernel approximations, under a fixed memory budget.