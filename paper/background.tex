\subsection{Random Fourier Features}
For shift-invariant kernels ($k(x,y) = \hat{k}(x-y)$), 
the random Fourier feature method constructs a 
random feature representation $z(x) \in \RR^m$ such that in expectation, $z(x)^T z(y) = k(x,y)$.
This construction is based on Bochner's Theorem, which states that any positive 
definite kernels is equal to the Fourier transform of a non-negative measure.
This allows for performing Monte Carlo approximations of this Fourier transform
in order to approximate the function.  The resulting features have the following
functional form: $z_i(x) = \sqrt{2/m}\cos(w_i^Tx + b_i)$, where
$w_i$ is drawn from the inverse Fourier transform of the kernel function $\hat{k}$,
and $b_i$ is drawn uniformly from $[0,2\pi]$. 

\subsection{\Nystrom method}
The \Nystrom method constructs a finite dimensional feature representation
$z(x) \in \RR^m$ such that $\Dotp{z(x),z(y)} \approx k(x,y)$.  It does this
by picking a set of landmark points $\{\hx_1,\ldots,\hx_m\} \in \cX$,
and taking the SVD $\hK = U\Lambda U^T$ of the $m$ by $m$ 
kernel matrix $\hK$ corresponding to these landmark points 
($\hK_{i,j} = k(\hx_i,\hx_j)$).  The \Nystrom representation for a point $x \in \cX$
is defined as $z(x) = \Lambda^{-1/2} U^T k_x$, where $k_x = [k(x,\hx_1),\ldots,k(x,\hx_m)]^T$.
Letting $K_{m,n} = [k_{x_1},\ldots,k_{x_n}] \in \RR^{m\times n}$, 
the \Nystrom method can be thought of as an efficient low-rank approximation
$K \approx K_{m,n}^T U \Lambda^{-1/2}\Lambda^{-1/2} U^T K_{m,n}$ of the full
$n$ by $n$ kernel matrix $K$ corresponding to the full dataset $\{x_i\}_{i=1}^n$.
One can also consider the lower dimension \Nystrom representation
$z_r(x) = \Lambda_r^{-1/2} U_r^T k_x \in \RR^r$, where only the top $r$ eigenvalues and
eigenvectors of $\hK$ are used, instead of all $m$.








\begin{itemize}
	\item Give a very brief overview of kernel approximation, Nystrom and RFF.
	\item Discuss the amount of memory each of these methods uses during training.  It is important to be very clear about how we are measuring memory.
\end{itemize}

\subsection{Avron Generalization Bound}
\begin{itemize}
	\item Present their bound, and discuss in moderate detail. 
	$$R(\tf) \leq \frac{1}{1-\Delta}\hat{R}_K(f) + \frac{\Delta}{1+\Delta} \cdot \frac{rank(\tK)}{n}\cdot \sigma_{\nu}^2$$ 
	\item Mention that we can understand $\Delta$ in terms of $D =\max_i |\log(\sigma_i + \lambda) - \log(\tsigma_i + \lambda)|$ ($1+\Delta \geq \exp(D)$), as a way of visualizing the bound; if you plot both spectra $+\lambda$ in log-scale, the $\Delta$ is the maximum gap between the two lines (if eigenvectors are the same.  So even if the eigenvectors are different, this us gives lower-bound for the true value of $\Delta$).
	\item Discuss their result which bounds the number of random Fourier features needed to get a specific $\Delta$ value.
\end{itemize}

\subsection{Notation}
\begin{itemize}
	\item Present notation for Nystrom and RFF features, kernel matrices, etc.  
	\item Specify that $n$ denotes number of datapoints, $d$ is data dimension, $m$ is number of random features, etc.
\end{itemize}