Our work is not the first to attempt to minimize the memory footprint of kernel approximation methods.  One line of work proposes using structured matrices in place of the projection matrix used by random Fourier features \citep{fastfood,yu15,sphereRKS}; these methods generally reduce the storage requirement for the projection matrix from $O(md)$ to $O(m)$, where $d$ is the dimension of the data. This work is orthogonal to ours, and in fact we use circulant projections \citep{yu15} in our implementation of LP-RFFs; importantly, these methods do not reduce the amount of memory occupied by the features themselves, or by the learned model parameters. Another line of work uses feature selection in order to reduce the number of random Fourier features required to attain a specific level of performance \citep{sparseRKS, may2016}; once again, our work is orthogonal to theirs, because feature selection can be performed on the low-precision features.  For the \Nystrom method, there has been extensive work on the best ways of choosing the landmark points \citep{kmeans08,kumar12,gittens13}.  Several other methods reduce the memory requirement for \Nystrom \cite{ensemble09,fastpred14,meka14}. In this paper, however, we focus our efforts on random Fourier features, which we observe perform significantly better than the \Nystrom method, under a fixed memory budget.

On the topic of scaling kernel methods to large datasets, there have been a number of notable recent papers.  From an algorithmic perspective, \citet{block16} propose a distributed block coordinate descent method for solving large-scale least squares problems using the \Nystrom method or random Fourier features. Although they focus on the distributed setting (1024 cores on 128 machines) and we do not, our contribution could nonetheless help lower the amount of memory and computation performed by each worker.  The recent work of \citet{may2017} uses a single GPU to train large RFF models for speech recognition, on datasets with up to 16 million training points and 5000 classes, showing comparable performance to deep neural networks. In that work they were limited by the number of features they could fit on a single GPU, and thus our work could also help them scale their experiments to even larger numbers of random features.

From a theoretical perspective, there has been a lot of recent work analyzing the generalization performance of kernel approximation methods \citep{bach13,alaoui15,rudi15,optrff15,musco17,rudi17,bach17,avron17}. This work is generally concerned with proving upper bounds on the number of features needed to attain a certain level of generalization performance, or designing new sampling methods for the random features in order to attain stronger bounds. The theoretical work most relevant to ours is that of \citet{avron17}, which bounds the generalization performance of kernel approximation methods in the kernel ridge regression context, in terms of a notion of distance to the exact kernel matrix.  While the theory is informative, the method proposed in the paper for sampling random features scales exponentially with the dimension of the data, making it impractical in most regimes. We build on their theoretical framework to propose a novel method for achieving $4x$ to $30x$ compression at no loss in performance, relative to full-precision RFFs with circulant projections. Furthermore, while they propose a metric to measure the distance between a kernel matrix and an approximation to it, they only measure this metric on a small synthetic dataset. We modify their metric and measure it across four datasets for various kernel approximation methods. With our modified definition, we demonstrate that this metric correlates very strongly with generalization performance in practice.

Our work also provides an important new perspective to the question of which kernel method is best: the \Nystrom method or RFFs. An in-depth comparison of these methods was performed by \citet{nysvsrff12}, and argues that from both theoretical and empirical perspectives, the \Nystrom method is preferable to RFFs, for a fixed number of features. We show, through larger scale experiments (more features, larger datasets), that when memory utilization is considered, RFFs are better.

At the core of our contribution is the idea of using low-precision to use fewer bits, while still retaining the desired statistical properties of the learning algorithm.  There has been much recent interest in the topic of low-precision for accelerating training and/or inference, as well as for compressing models \citep{gupta15,hogwild15,hubara16,halp18,desa17,han15}.  From a hardware perspective, there has also been significant progress in developing chips which support low-precision operations, in order to use less energy, and perform computations faster; notable examples are Google's TPU \citep{tpu17} and Microsoft's Project Brainwave \citep{brainwave17}. Our work benefits enormously from these recent efforts, as these hardware accelerators could dramatically speed up the execution time of the training algorithm described in this work.