%\begin{itemize}
%	\item We have shown the practical significance of the recent theory of \citep{avron17}.  This came in two parts: (1) It explains why RFF outperforms \NystromNS, even when they both have the same kernel approximation error; a direct consequence is that under a fixed memory budget, RFF is better. (2) It inspired us to define, analyze, and test the performance of LP-RFF.
%	\item We showed that we can do much better, for a fixed memory budget, using LP-RFF, because they are a much higher-rank decomposition of the kernel matrix.
%	\item This opens the door for larger-scale kernel approximation experiments, which can leverage recent advances in chips, etc, for fast training.  For future work, we plan to implement efficient version of our LP-RFF training, to be able to apply this to large/challenging tasks (\eg, speech recognition, computer vision).
%\end{itemize}


We believe these contributions provide fundamental insight into the field of kernel approximation, and open the door to scaling kernel approximation methods to much larger numbers of features, and thus improved generalization performance. This will require implementing LP-RFFs on fast hardware (\eg, GPUs, TPUs, FPGAs) that leverages low-precision for faster and more energy efficient computation.  We hope to use these advances to further study the limits of kernel approximation methods, and compare their performance with DNNs, across various domains, such as speech recognition \citep{may2017}.