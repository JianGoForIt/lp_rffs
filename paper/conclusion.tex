We have proposed using low-precision as a way of dramatically reducing the memory footprint for mini-batch training of kernel approximation models.  We analyze the effect on generalization performance of the quantization noise introduced by low-precision, showing there are important regimes in which precision can be reduced dramatically, with minimal effect on generalization performance.  Empirically, we showed across four datasets, including TIMIT, that we can achieve $4x$ to $30x$ smaller memory footprint relative to circulant RFFs, without impacting generalization performance.  We believe these contributions provide fundamental insight into the field of kernel approximation, and open the door to scaling kernel approximation methods to much larger numbers of features, and thus improved generalization performance. This will require implementing LP-RFFs on fast hardware (\eg, GPUs, TPUs, FPGAs) that leverages low-precision for faster and more energy efficient computation.  We hope to use these advances to further study the limits of kernel approximation methods, and compare their performance with DNNs, across various domains, such as speech recognition \citep{may2017}.