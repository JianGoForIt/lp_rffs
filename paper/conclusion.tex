We have proposed using low-precision as a way of reducing the memory footprint for mini-batch training of kernel approximation models. We showed that both theoretically and empirically, LP-RFFs can be used to attain improved performance per bit. We believe these contributions provide fundamental insight into the field of kernel approximation, and open the door to scaling kernel approximation methods to much larger numbers of features, and thus improved generalization performance. We hope to use these advances to further study the limits of kernel approximation methods, and compare performance with DNNs, across domains such as speech recognition \citep{may2017}.

%This will require implementing LP-RFFs on fast hardware (\eg, GPUs, TPUs, FPGAs) that leverages low-precision for faster and more energy efficient computation.  We hope to use these advances to further study the limits of kernel approximation methods, and compare their performance with DNNs, across various domains, such as speech recognition \citep{may2017}.