\begin{itemize}
	\item We have shown the practical significance of the recent theory of \citep{avron17}.  This came in two parts: (1) It explains why RFF outperforms \NystromNS, even when they both have the same kernel approximation error; a direct consequence is that under a fixed memory budget, RFF is better. (2) It inspired us to define, analyze, and test the performance of LP-RFF.
	\item We showed that we can do much better, for a fixed memory budget, using LP-RFF, because they are a much higher-rank decomposition of the kernel matrix.
	\item This opens the door for larger-scale kernel approximation experiments, which can leverage recent advances in chips, etc, for fast training.  For future work, we plan to implement efficient version of our LP-RFF training, to be able to apply this to large/challenging tasks (\eg, speech recognition, computer vision).
\end{itemize}